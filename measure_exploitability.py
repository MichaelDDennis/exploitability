from sacred import Experiment
from sacred.observers import FileStorageObserver

from tqdm import tqdm

import ray
from ray.tune.logger import pretty_print

from env_utils import get_env

from alg_utils import get_algs, get_new_train, policy_mapping_fn

from marl_training import sync_trainers
from marl_training import SAVE_DIR as TRAINING_SAVE_DIR

from sacred import SETTINGS

from glob import glob
import os
import numpy as np
from copy import deepcopy
import pdb

SETTINGS['CAPTURE_MODE'] = 'sys'

EXPLOIT_SAVE_DIR = "exploitability_testing"

exploitability_ex = Experiment("measure_exploitability")
exploitability_ex.observers.append(FileStorageObserver(EXPLOIT_SAVE_DIR))


@exploitability_ex.config
def my_config():
    # TODO only have one of these, since they're fully determined from each other
    adv_index = 0
    victim_index = 1
    adversary_alg = "pg"
    victim_alg = "pg"
    env = "tictactoe-v0"
    training_iters = 1000
    samples = 100
    dim = 2
    prev_run = _latest_training_sacred_dir()


@exploitability_ex.named_config
def tiny_testing():
    training_iters = 4
    samples = 2

@exploitability_ex.named_config
def core_frozen():
    """
    Reinitializes redundant weights, resets core weights to pretrained values and freezes
    """
    adversary_alg = "pg_hierarchical_core_frozen"
    reeinit_weights_keywords = ['redundant']



@exploitability_ex.named_config
def redundant_frozen():
    """
    Reinitializes core weights, resets redundant weights to pretrained values and freezes
    """
    adversary_alg = "pg_hierarchical_redundant_frozen"
    reeinit_weights_keywords = ['core']

@exploitability_ex.named_config
def redundant_reinit_core_finetune():
    """
    Reinitializes redundant weights, resets core weights to pretrained values, trains both (nothing frozen)
    """
    adversary_alg = "pg_hierarchical"
    reeinit_weights_keywords = ['redundant']

@exploitability_ex.named_config
def core_reinit_redundant_finetune():
    """
    Reinitializes core weights, resets redundant weights to pretrained values, trains both (nothing frozen)
    """
    adversary_alg = "pg_hierarchical"
    reeinit_weights_keywords = ['core']

def _safe_int(st):
    try:
        return int(st)
    except ValueError:
        return -1

def _latest_training_sacred_dir():
    """
    :return: A string of the Sacred directory with highest numeric value within TRAINING_SAVE_DIR
    """
    subdirs = glob(f'{TRAINING_SAVE_DIR}/*')
    subdir_ints = [_safe_int(subdir.split('/')[1]) for subdir in subdirs]
    return str(max(subdir_ints))

@exploitability_ex.capture
def _get_checkpoint_from_run(prev_run):
    """
    Gets the checkpoint path given a Sacred dir; defaults to the highest numeric value checkpoint if multiple are found
    :param prev_run: An int or string of the sacred run directory we want to find a checkpoint within
    :return: The full path to a checkpoint within a sacred training dir
    """
    base_path = os.path.join(TRAINING_SAVE_DIR, str(prev_run), 'checkpoints')
    subdirs = glob(os.path.join(base_path, 'checkpoint_*'))
    if len(subdirs) == 0:
        raise ValueError("No checkpoint subdirs found")
    if len(subdirs) == 1:
        subdir = subdirs[0]
    else:
        subdir = max(subdirs, key=lambda x: _safe_int(x.split('-')[-1]))

    subfiles = [el for el in glob(os.path.join(subdir, 'checkpoint-[0-9]*')) if 'tune_metadata' not in el]
    if len(subfiles) != 1:
        print(subfiles)
        raise ValueError(f"Encountered an unexpected number of checkpoints inside folder {os.path.join(base_path, subdir)}")
    return subfiles[0]

def get_training_policies(trainers):
    """
    Returns the policies that are being actively trained by each trainer
    """
    return [trainers[ind].get_policy(policy_mapping_fn(ind)) for ind in range(len(trainers))]

def get_training_weights(trainers):
    """
    Returns the weights of the policies that are being actively trained by each trainer
    """
    return [trainers[ind].get_weights(policy_mapping_fn(ind))[policy_mapping_fn(ind)] for ind in range(len(trainers))]


def _check_correct_weight_trainability(pretrain_weights, posttrain_weights, ind,
                                       should_not_train='core', should_train='redundant'):
    """
    A testing function, to check whether weights that should be frozen are staying frozen, and weights that should be
    training are changing.
    :param pretrain_weights: The result of deepcopy(get_training_weights(trainers)) before training
    :param posttrain_weights: The result of deepcopy(get_training_weights(trainers)) after training
    :param ind: The index of the policy you want to check
    :param should_not_train: String, `core` or `redundant`, indicating weights that should be frozen. If None, assumes no weights frozen.
    :param should_train: String, `core` or `redundant`, indicating weights that should be trainable. If None, assumes all weights frozen.
    """
    sample_variable = {
        "core": f"{policy_mapping_fn(ind)}/core_move_policy/fc_core_0/kernel",
        "redundant": f"{policy_mapping_fn(ind)}/redundant_move_policy/fc_redundant_0/kernel"
    }
    if should_not_train is not None:
        no_train_before = pretrain_weights[ind][sample_variable[should_not_train]]
        no_train_after = posttrain_weights[ind][sample_variable[should_not_train]]
        assert np.allclose(no_train_before, no_train_after)

    if should_train is not None:
        train_before = pretrain_weights[ind][sample_variable[should_train]]
        train_after = posttrain_weights[ind][sample_variable[should_train]]
        assert not np.allclose(train_before, train_after)

def remove_weights_by_keyword(kw, weights):
    """
    Returns a copy of weights dictionary that only keeps weights that do not contain keyword `kw` within their key.
    """
    keys_to_keep = [k for k in weights.keys() if kw not in k]
    return {k:v for k, v in weights.items() if k in keys_to_keep}


# Iterative Adversarial Training PseudoCode
# Take some (optionally?) pretrained policy P
# Repeat k times:
# --> Fix weights of P
# --> Train adversary A against P, with P unable to update weights, for <some number> of steps. Adversary either from scratch or from pretrain
# --> Fix weights of A
# --> Train P against A, with A unable to update weights, for <some number> of steps

@exploitability_ex.capture
def get_pretrained_weight_subset(trainer, adv_index, reeinit_weights_keywords):
    # Figure out which string key corresponds to the policy in the adversarial position
    adv_policy_key = policy_mapping_fn(adv_index)

    # Pull out initial weights, and remove those we want to re-initialize rather than use pretrained values for
    initial_adversary_weights = deepcopy(trainer.get_weights(adv_policy_key))[adv_policy_key]
    for kw in reeinit_weights_keywords:
        initial_adversary_weights = remove_weights_by_keyword(kw, initial_adversary_weights)
    return {adv_policy_key: initial_adversary_weights}

@exploitability_ex.capture
def get_agent_algorithms(victim_index, adv_index, victim_alg, adversary_alg):
    agent_algorithms = [None, None]
    agent_algorithms[victim_index] = victim_alg
    agent_algorithms[adv_index] = adversary_alg
    return agent_algorithms

# @exploitability_ex.capture
# def init_and_get_algorithms(env, dim):
#
# @exploitability_ex.command
# def iterative_adv_train():


@exploitability_ex.automain
def my_main(_run, dim, env, victim_index, adv_index, samples, training_iters, reeinit_weights_keywords):
    ray.init()
    obs_space, act_space, env = get_env(env, dim)
    agent_algorithms = get_agent_algorithms()
    # First, we create the trainers, both of which should have agent1_policy use a frozen model
    trainers, pols = get_algs(agent_algorithms, obs_space, act_space, env)
    checkpoint_path = _get_checkpoint_from_run()

    # We specifically restore weights of the non-training victim trainer
    trainers[victim_index].restore(checkpoint_path)

    # It's a little weird that we're pulling the adversary weights out of the victim-index trainer,
    # because that's the one we're restoring from pretrained values in this setting, #TODO make this cleaner
    pruned_adversary_weights = get_pretrained_weight_subset(trainers[victim_index])

    for t in range(samples):
        # Create a new trainer object for the adversary
        adv = get_new_train(agent_algorithms[adv_index], env, pols, adv_index)

        # Restore any weights we want to reset to their pretrained values
        adv.set_weights(pruned_adversary_weights)

        trainers[adv_index] = adv
        # add silent
        sync_trainers(trainers)
        for _ in tqdm(range(training_iters)):
            sync_trainers(trainers)
            adv.train()

    sync_trainers(trainers)

    trainers[adv_index].save(EXPLOIT_SAVE_DIR + "/" + str(_run._id) + "/checkpoints/")