from sacred import Experiment
from sacred.observers import FileStorageObserver

from tqdm import tqdm

import ray
from ray.tune.logger import pretty_print

from env_utils import get_env

from alg_utils import get_single_trainer, policy_mapping_fn, get_policies

from marl_training import sync_trainers
from marl_training import SAVE_DIR as TRAINING_SAVE_DIR

from sacred import SETTINGS

from glob import glob
import os
import numpy as np
from copy import deepcopy
from functools import partial

import tqdm

import pdb

SETTINGS['CAPTURE_MODE'] = 'sys'

EXPLOIT_SAVE_DIR = "exploitability_testing"

exploitability_ex = Experiment("measure_exploitability")
exploitability_ex.observers.append(FileStorageObserver(EXPLOIT_SAVE_DIR))


@exploitability_ex.config
def no_victim_finetuning():
    # TODO only have one of these, since they're fully determined from each other
    adv_index = 1
    victim_index = 0
    adversary_alg = "pg"
    victim_alg = "pg"
    env = "tictactoe-v0"
    adversary_training_iters = 1000
    victim_finetuning_iters = 0
    outer_loop_iters = 100
    dim = 2
    # Note that this currently isn't smart enough to ensure that your adversary and victim algs during training are
    # compatible with the ones you've chosen here, and they need to be
    prev_run = _latest_training_sacred_dir()
    victim_trainer_func = partial(possibly_pretrained_trainer, load_weights=True) # victim reloads all weights
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=False) #adversary reloads no weights


@exploitability_ex.named_config
def small_iterative_training_test():
    adversary_training_iters = 3
    victim_finetuning_iters = 3
    outer_loop_iters = 4

@exploitability_ex.named_config
def small_test():
    adversary_training_iters = 4
    outer_loop_iters = 2

@exploitability_ex.named_config
def core_frozen():
    """
    Reinitializes redundant weights, resets core weights to pretrained values and freezes
    """
    adversary_alg = "pg_hierarchical_core_frozen"
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True,
                                     pretrained_weight_keywords=['core'])

@exploitability_ex.named_config
def redundant_frozen():
    """
    Reinitializes core weights, resets redundant weights to pretrained values and freezes
    """
    adversary_alg = "pg_hierarchical_redundant_frozen"
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True,
                                     pretrained_weight_keywords=['redundant'])

@exploitability_ex.named_config
def redundant_reinit_core_finetune():
    """
    Reinitializes redundant weights, resets core weights to pretrained values, trains both (nothing frozen)
    """
    adversary_alg = "pg_hierarchical"
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True,
                                     pretrained_weight_keywords=['core'])

@exploitability_ex.named_config
def core_reinit_redundant_finetune():
    """
    Reinitializes core weights, resets redundant weights to pretrained values, trains both (nothing frozen)
    """
    adversary_alg = "pg_hierarchical"
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True,
                                     pretrained_weight_keywords=['redundant'])


@exploitability_ex.named_config
def paper_config_tot_vul():
    victim_alg = "pg"
    adversary_alg="pg_hierarchical_1by256"
    adversary_training_iters=800
    outer_loop_iters=10


@exploitability_ex.named_config
def paper_config_sys_vul():
    victim_alg = "pg"
    adversary_alg = "pg_hierarchical_core_frozen_1by256"
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True,
                                     pretrained_weight_keywords=['core'])
    adversary_training_iters=800
    outer_loop_iters=10


@exploitability_ex.named_config
def paper_config_strat_vul():
    victim_alg = "pg"
    adversary_alg = "pg_hierarchical_redundant_frozen_1by256"
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True,
                                     pretrained_weight_keywords=['redundant'])
    adversary_training_iters = 800
    outer_loop_iters = 10


@exploitability_ex.named_config
def paper_config_itr_adv_train1():
    victim_alg = "pg"
    adversary_alg = "pg_hierarchical_1by256"
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True,
                                     pretrained_weight_keywords=['redundant'])
    adversary_training_iters = 500
    outer_loop_iters = 500
    victim_finetuning_iters = 20

    pass

@exploitability_ex.named_config
def paper_config_itr_adv_train():
    victim_alg = "pg"
    adversary_alg = "pg_hierarchical_1by256"
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True,
                                     pretrained_weight_keywords=['redundant'])
    adversary_training_iters = 500
    outer_loop_iters = 500
    victim_finetuning_iters = 20


@exploitability_ex.named_config
def paper_config_itr_adv_train1():
    victim_alg = "pg"
    adversary_alg = "pg_hierarchical_1by256"
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True,
                                     pretrained_weight_keywords=['redundant'])
    adversary_training_iters = 500
    outer_loop_iters = 500
    victim_finetuning_iters = 10


@exploitability_ex.named_config
def paper_config_itr_adv_train2():
    victim_alg = "pg"
    adversary_alg = "pg_hierarchical_1by256"
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True,
                                     pretrained_weight_keywords=['redundant'])
    adversary_training_iters = 250
    outer_loop_iters = 500
    victim_finetuning_iters = 20


@exploitability_ex.named_config
def paper_config_itr_adv_train3():
    victim_alg = "pg"
    adversary_alg = "pg_hierarchical_1by256"
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True,
                                     pretrained_weight_keywords=['redundant'])
    adversary_training_iters = 250
    outer_loop_iters = 500
    victim_finetuning_iters = 10


def _safe_int(st):
    try:
        return int(st)
    except ValueError:
        return -1

def _latest_training_sacred_dir():
    """
    :return: A string of the Sacred directory with highest numeric value within TRAINING_SAVE_DIR
    """
    subdirs = glob(f'{TRAINING_SAVE_DIR}/*')
    subdir_ints = [_safe_int(subdir.split('/')[1]) for subdir in subdirs]
    return str(max(subdir_ints))

@exploitability_ex.capture
def _get_checkpoint_from_run(prev_run, index, dir_loc=TRAINING_SAVE_DIR):
    """
    Gets the checkpoint path given a Sacred dir; defaults to the highest numeric value checkpoint if multiple are found
    :param prev_run: An int or string of the sacred run directory we want to find a checkpoint within
    :return: The full path to a checkpoint within a sacred training dir
    """
    base_path = os.path.join(dir_loc, str(prev_run), 'checkpoints', str(index))
    subdirs = glob(os.path.join(base_path, 'checkpoint_*'))
    if len(subdirs) == 0:
        raise ValueError("No checkpoint subdirs found")
    if len(subdirs) == 1:
        subdir = subdirs[0]
    else:
        subdir = max(subdirs, key=lambda x: _safe_int(x.split('-')[-1]))

    subfiles = [el for el in glob(os.path.join(subdir, 'checkpoint-[0-9]*')) if 'tune_metadata' not in el]
    if len(subfiles) != 1:
        print(subfiles)
        raise ValueError(f"Encountered an unexpected number of checkpoints inside folder {os.path.join(base_path, subdir)}")
    return subfiles[0]

def get_training_policies(trainers):
    """
    Returns the policies that are being actively trained by each trainer
    """
    return [trainers[ind].get_policy(policy_mapping_fn(ind)) for ind in range(len(trainers))]

def get_training_weights(trainers):
    """
    Returns the weights of the policies that are being actively trained by each trainer
    """
    return [trainers[ind].get_weights(policy_mapping_fn(ind))[policy_mapping_fn(ind)] for ind in range(len(trainers))]


def _check_correct_weight_trainability(pretrain_weights, posttrain_weights, ind,
                                       should_not_train='core', should_train='redundant'):
    """
    A testing function, to check whether weights that should be frozen are staying frozen, and weights that should be
    training are changing.
    :param pretrain_weights: The result of deepcopy(get_training_weights(trainers)) before training
    :param posttrain_weights: The result of deepcopy(get_training_weights(trainers)) after training
    :param ind: The index of the policy you want to check
    :param should_not_train: String, `core` or `redundant`, indicating weights that should be frozen. If None, assumes no weights frozen.
    :param should_train: String, `core` or `redundant`, indicating weights that should be trainable. If None, assumes all weights frozen.
    """
    sample_variable = {
        "core": f"{policy_mapping_fn(ind)}/core_move_policy/fc_core_0/kernel",
        "redundant": f"{policy_mapping_fn(ind)}/redundant_move_policy/fc_redundant_0/kernel"
    }
    if should_not_train is not None:
        no_train_before = pretrain_weights[ind][sample_variable[should_not_train]]
        no_train_after = posttrain_weights[ind][sample_variable[should_not_train]]
        assert np.allclose(no_train_before, no_train_after)

    if should_train is not None:
        train_before = pretrain_weights[ind][sample_variable[should_train]]
        train_after = posttrain_weights[ind][sample_variable[should_train]]
        assert not np.allclose(train_before, train_after)



# Iterative Adversarial Training PseudoCode
# Take some (optionally?) pretrained policy P
# Repeat k times:
# --> Fix weights of P
# --> Train adversary A against P, with P unable to update weights, for <some number> of steps. Adversary either from scratch or from pretrain
# --> Fix weights of A
# --> Train P against A, with A unable to update weights, for <some number> of steps



@exploitability_ex.capture
def get_agent_algorithms(victim_index, adv_index, victim_alg, adversary_alg):
    agent_algorithms = [None, None]
    agent_algorithms[victim_index] = victim_alg
    agent_algorithms[adv_index] = adversary_alg
    return agent_algorithms

# @exploitability_ex.capture
# def init_and_get_algorithms(env, dim):
#
# @exploitability_ex.command
# def iterative_adv_train():

def get_pretrained_weight_subset(trainer, index, pretrained_weight_keywords):
    def any_keyword_in_key(keywords, key):
        for kw in keywords:
            if kw in key:
                return True
        return False

        # Figure out which string key corresponds to the policy in the adversarial position
    assert pretrained_weight_keywords is not None
    policy_key = policy_mapping_fn(index)

    # Pull out initial weights, and remove those we want to re-initialize rather than use pretrained values for
    initial_weights = deepcopy(trainer.get_weights(policy_key))[policy_key]
    initial_weights = {k: v for k, v in initial_weights.items()
                       if any_keyword_in_key(pretrained_weight_keywords, k)}

    return {policy_key: initial_weights}


def possibly_pretrained_trainer(alg, ind, policies, env, logdir, checkpoint_path=None,
                                load_weights=True, pretrained_weight_keywords=None):
    # If load_weights=True pretrained_weight_keywords is None, all weights are kept
    # If load_weights=False, a fully reinitialized trainer is returned

    trainer = get_single_trainer(alg, policies, ind, env, logdir)
    initial_random_weights = trainer.get_weights(policy_mapping_fn(ind))

    if not load_weights:
        # if we're not loading any weights, return a randomly initialized trainer
        return trainer

    trainer.restore(checkpoint_path)
    if pretrained_weight_keywords is None:
        # if we're not subsetting out specific weights to keep, then don't calculate a weight mask
        return trainer

    weight_mask = get_pretrained_weight_subset(trainer, ind, pretrained_weight_keywords)
    trainer.set_weights(initial_random_weights)
    trainer.set_weights(weight_mask)
    return trainer

@exploitability_ex.automain
def my_main(_run, dim, env,prev_run, victim_index, adv_index, outer_loop_iters, adversary_training_iters, victim_finetuning_iters,
            victim_trainer_func, adversary_trainer_func):
    ray.init()
    logdir = EXPLOIT_SAVE_DIR + "/" + str(_run._id) + "/rllib/"

    obs_space, act_space, env = get_env(env, dim)
    agent_algorithms = get_agent_algorithms()
    # First, we create the trainers, both of which should have agent1_policy use a frozen model
    policies = get_policies(agent_algorithms, obs_space, act_space)
    trainers = [None, None]
    trainers[victim_index] = victim_trainer_func(agent_algorithms[victim_index], victim_index, policies, env, logdir, _get_checkpoint_from_run(prev_run,victim_index))


    # This framework should allow for doing both exploitability testing and iterative adversarial training in the same framework.
    # If you want to do straight exploitability testing, set victim_finetuning_iters to 0
    # If you want to do iterative adversarial training, set adversary_training_iters and victim_finetuning_iters to the
    # number of steps you want to train the adversary and victim against fixed versions of each other in the inner loop
    # Note that adversary is current reinitialized in the same way in each iteration of the loop

    for t in range(outer_loop_iters):
        print(f"Outer loop iter: {t}/{outer_loop_iters}, {(t/outer_loop_iters)*100}% complete")
        # Create a new trainer object for the adversary
        # TODO allow for an adversary that stays the same across outer_loop iterations
        trainers[adv_index] = adversary_trainer_func(agent_algorithms[adv_index], adv_index, policies,  env, logdir, _get_checkpoint_from_run(prev_run, adv_index))
        sync_trainers(trainers)
        print("Training adversary")
        for _ in tqdm(range(adversary_training_iters)):
            sync_trainers(trainers)
            trainers[adv_index].train()

        print("Finetuning victim against adversary")
        for _ in tqdm(range(victim_finetuning_iters)):
            sync_trainers(trainers)
            trainers[victim_index].train()

        sync_trainers(trainers)
        trainers[adv_index].save(os.path.join(EXPLOIT_SAVE_DIR, str(_run._id), "checkpoints", f"iter_{t}/"))
        # TODO calculate performance here and save out in the same directory