from sacred import Experiment
from sacred.observers import FileStorageObserver

from tqdm import tqdm

import ray
from ray.tune.logger import pretty_print

from env_utils import get_env

from alg_utils import get_algs, get_new_train, policy_mapping_fn

from marl_training import sync_trainers
from marl_training import SAVE_DIR as TRAINING_SAVE_DIR

from sacred import SETTINGS

from glob import glob
import os
import numpy as np
from copy import deepcopy
import pdb

SETTINGS['CAPTURE_MODE'] = 'sys'

EXPLOIT_SAVE_DIR = "exploitability_testing"

exploitability_ex = Experiment("measure_exploitability")
exploitability_ex.observers.append(FileStorageObserver(EXPLOIT_SAVE_DIR))


@exploitability_ex.config
def my_config():
    agent1_alg = "pg"
    agent2_alg = "pg"
    env = "tictactoe-v0"
    training_iters = 1000
    samples = 100
    dim = 2
    silent = True
    prev_run = _latest_sacred_dir()


@exploitability_ex.named_config
def core_frozen():
    agent1_alg = "pg_custom_core_frozen"
    training_iters = 5
    samples = 2

@exploitability_ex.named_config
def redundant_frozen():
    agent1_alg = "pg_custom_redundant_frozen"
    training_iters = 5
    samples = 2

VIC_INDEX = 1
ADV_INDEX = 0

def _safe_int(st):
    try:
        return int(st)
    except ValueError:
        return -1

def _latest_sacred_dir():
    subdirs = glob(f'{TRAINING_SAVE_DIR}/*')
    subdir_ints = [_safe_int(subdir.split('/')[1]) for subdir in subdirs]
    return str(max(subdir_ints))

@exploitability_ex.capture
def _get_checkpoint_from_run(prev_run):
    base_path = os.path.join(TRAINING_SAVE_DIR, str(prev_run), 'checkpoints')
    subdirs = glob(os.path.join(base_path, 'checkpoint_*'))
    if len(subdirs) == 0:
        raise ValueError("No checkpoint subdirs found")
    if len(subdirs) == 1:
        subdir = subdirs[0]
    else:
        subdir = max(subdirs, key=lambda x: _safe_int(x.split('-')[-1]))

    subfiles = [el for el in glob(os.path.join(subdir, 'checkpoint-[0-9]*')) if 'tune_metadata' not in el]
    if len(subfiles) != 1:
        print(subfiles)
        raise ValueError(f"Encountered an unexpected number of checkpoints inside folder {os.path.join(base_path, subdir)}")
    return subfiles[0]

def get_training_policies(trainers):
    # TODO to be fixed if we change the policy keys
    return [trainers[ind].get_policy(policy_mapping_fn(ind)) for ind in range(len(trainers))]

def get_training_weights(trainers):
    return [trainers[ind].get_weights(policy_mapping_fn(ind))[policy_mapping_fn(ind)] for ind in range(len(trainers))]


@exploitability_ex.automain
def my_main(_run, agent1_alg, dim, agent2_alg, env, samples, training_iters, silent):
    ray.init()
    obs_space, act_space, env = get_env(env, dim)
    agent_algorithms = [agent1_alg, agent2_alg]

    trainers, pols = get_algs(agent_algorithms, obs_space, act_space, env)

    checkpoint_path = _get_checkpoint_from_run()
    # First, we create the trainers, both of which should have agent1_policy use a frozen model

    trainers[VIC_INDEX].restore(checkpoint_path)
    # We specifically restore weights of the non-training victim trainer

    # Woo, Notional Pseudocode
    # 1. Create trainers with the correct policies
    # 2. Create a new
    initial_alg_1_weights = deepcopy(trainers[ADV_INDEX].get_weights('agent1_policy'))
    for t in range(samples):
        # We then create a new trainer object for the adversary, not restoring weights
        adv = get_new_train(agent_algorithms[ADV_INDEX], env, pols, ADV_INDEX)
        trainers[ADV_INDEX] = adv
        # add silent
        sync_trainers(trainers)
        pretrain_weights = deepcopy(get_training_weights(trainers))
        for _ in tqdm(range(training_iters)):
            sync_trainers(trainers)
            adv.train()
        posttrain_weights = deepcopy(get_training_weights(trainers))
        core_kernel_before = pretrain_weights[0]["agent1_policy/core_move_policy/fc_core_0/kernel"]
        core_kernel_after = posttrain_weights[0]["agent1_policy/core_move_policy/fc_core_0/kernel"]

        redundant_kernel_before = pretrain_weights[0]["agent1_policy/redundant_move_policy/fc_redundant_0/kernel"]
        redundant_kernel_after = posttrain_weights[0]["agent1_policy/redundant_move_policy/fc_redundant_0/kernel"]

        # Okay, looks like setting trainable to False here isn't working for some reason. Sigh.
        pdb.set_trace()

    sync_trainers(trainers)

    trainers[1].save(EXPLOIT_SAVE_DIR + "/" + str(_run._id) + "/checkpoints/")

# TODO maybe at some point change the policy names to just be indices