from sacred import Experiment
from sacred.observers import FileStorageObserver

from tqdm import tqdm

import ray
from ray.tune.logger import pretty_print

from env_utils import get_env

from alg_utils import get_single_trainer, policy_mapping_fn, get_pg_train, get_pg_pol, get_hierarchical_pg_pol, get_policies

from marl_training import sync_trainers
from marl_training import SAVE_DIR as TRAINING_SAVE_DIR

from sacred import SETTINGS

from glob import glob
import os
import numpy as np
from copy import deepcopy
from functools import partial

SETTINGS['CAPTURE_MODE'] = 'sys'

EXPLOIT_SAVE_DIR = "exploitability_testing"

exploitability_ex = Experiment("measure_exploitability")
exploitability_ex.observers.append(FileStorageObserver(EXPLOIT_SAVE_DIR))


@exploitability_ex.config
def no_victim_finetuning():
    # TODO only have one of these, since they're fully determined from each other
    adv_index = 1
    victim_index = 0
    gamma = 0.95
    adversary_policy = get_pg_pol
    victim_policy = get_pg_pol
    adversary_trainer = partial(get_pg_train, gamma=gamma)
    victim_trainer = partial(get_pg_train, gamma=gamma)
    victim_policy_key = "baseline"
    adversary_type_key = "total"
    dir_loc = TRAINING_SAVE_DIR
    env = "tictactoe-v0"
    adversary_training_iters = 800
    victim_finetuning_iters = 0
    outer_loop_iters = 100
    dim = 10
    # Note that this currently isn't smart enough to ensure that your adversary and victim algs during training are
    # compatible with the ones you've chosen here, and they need to be
    prev_run = latest_training_sacred_dir(dir_loc)
    victim_trainer_func = partial(possibly_pretrained_trainer, load_weights=True) # victim reloads all weights
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=False) #adversary reloads no weights
    dir_loc = TRAINING_SAVE_DIR
    reload_checkpoint_index = None

@exploitability_ex.named_config
def reload_from_adv_ckpt():
    # TODO figure out a way to not have to reset adv_index here
    adv_index = 1
    reload_checkpoint_index = adv_index

@exploitability_ex.named_config
def keep_all_weights():
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True)
    dim = 2

@exploitability_ex.named_config
def small_iterative_training_test():
    adversary_training_iters = 3
    victim_finetuning_iters = 3
    outer_loop_iters = 4

@exploitability_ex.named_config
def small_test():
    adversary_training_iters = 4
    outer_loop_iters = 3

@exploitability_ex.named_config
def core_frozen():
    """
    Reinitializes redundant weights, resets core weights to pretrained values and freezes
    """
    adversary_policy = partial(get_hierarchical_pg_pol, core_trainable=False)
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True,
                                     pretrained_weight_keywords=['core'])
    adversary_type_key = "systemic"
    dim=10
    adversary_training_iters = 1
    victim_finetuning_iters = 0
    outer_loop_iters = 10

@exploitability_ex.named_config
def redundant_frozen():
    """
    Reinitializes core weights, resets redundant weights to pretrained values and freezes
    """
    adversary_policy = partial(get_hierarchical_pg_pol, core_policy_hiddens=(256,), redundant_policy_hiddens=(256,),
                               redundant_trainable=False)

    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True,
                                     pretrained_weight_keywords=['redundant'])
    adversary_type_key = "strategic"
    adversary_training_iters = 1
    victim_finetuning_iters = 0
    outer_loop_iters = 10

@exploitability_ex.named_config
def paper_strat_ablation():
    """
    Reinitializes redundant weights, resets core weights to pretrained values, trains both (nothing frozen)
    """
    adversary_policy = partial(get_hierarchical_pg_pol, core_policy_hiddens=(256,), redundant_policy_hiddens=(256,))
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True,
                                     pretrained_weight_keywords=['redundant'])
    adversary_type_key = "total_redundant_pretrained"

@exploitability_ex.named_config
def paper_sys_ablation():
    """
    Reinitializes core weights, resets redundant weights to pretrained values, trains both (nothing frozen)
    """
    adversary_policy = partial(get_hierarchical_pg_pol, core_policy_hiddens=(256,), redundant_policy_hiddens=(256,))
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True,
                                     pretrained_weight_keywords=['core'])
    adversary_type_key = "total_core_pretrained"


@exploitability_ex.named_config
def paper_config_vul_tot():
    adversary_policy=partial(get_hierarchical_pg_pol, core_policy_hiddens=(256,), redundant_policy_hiddens=(256,))
    adversary_training_iters=800
    outer_loop_iters=10
    adversary_type_key = "total"
    dir_loc=None


@exploitability_ex.named_config
def paper_config_vul_sys():
    adversary_policy=partial(get_hierarchical_pg_pol, core_policy_hiddens=(256,), redundant_policy_hiddens=(256,),
                             core_trainable=False)
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True,
                                     pretrained_weight_keywords=['core'])
    adversary_training_iters=800
    outer_loop_iters=10
    adversary_type_key = "systemic"
    dir_loc=None



@exploitability_ex.named_config
def paper_config_vul_str():
    adversary_policy = partial(get_hierarchical_pg_pol, core_policy_hiddens=(256,), redundant_policy_hiddens=(256,),
                               redundant_trainable=False)
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True,
                                     pretrained_weight_keywords=['redundant'])
    adversary_training_iters = 800
    outer_loop_iters = 10
    adversary_type_key = "strategic"
    dir_loc=None


@exploitability_ex.named_config
def paper_config_itr_adv_train_tot():
    adversary_policy = partial(get_hierarchical_pg_pol, core_policy_hiddens=(256,), redundant_policy_hiddens=(256,))
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=False)
    adversary_training_iters = 800
    outer_loop_iters = 10
    victim_finetuning_iters = 10
    adversary_type_key = "iat_total"
    dir_loc = None

@exploitability_ex.named_config
def paper_config_itr_adv_train_sys():
    adversary_policy = partial(get_hierarchical_pg_pol, core_policy_hiddens=(256,), redundant_policy_hiddens=(256,),
                               core_trainable=False)
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True,
                                     pretrained_weight_keywords=['core'])
    adversary_training_iters = 800
    outer_loop_iters = 10
    victim_finetuning_iters = 10
    adversary_type_key = "iat_systemic"
    dir_loc = None

@exploitability_ex.named_config
def paper_config_itr_adv_train_str():
    adversary_policy = partial(get_hierarchical_pg_pol, core_policy_hiddens=(256,), redundant_policy_hiddens=(256,),
                               redundant_trainable=False)
    adversary_trainer_func = partial(possibly_pretrained_trainer, load_weights=True,
                                     pretrained_weight_keywords=['redundant'])
    adversary_training_iters = 800
    outer_loop_iters = 10
    victim_finetuning_iters = 10
    adversary_type_key = "iat_strategic"
    dir_loc = None

def _safe_int(st):
    try:
        return int(st)
    except ValueError:
        return -1

def latest_training_sacred_dir(policy_dir, max_val=np.inf):
    """
    :return: A string of the Sacred directory with highest numeric value within policy_dir
    """
    subdirs = glob(f'{policy_dir}/*')
    subdir_ints = [_safe_int(subdir.split('/')[1]) for subdir in subdirs]
    subdir_ints = [val if val < max_val else -1 for val in subdir_ints]
    return str(max(subdir_ints))

@exploitability_ex.capture
def _get_checkpoint_from_run(index, prev_run, dir_loc):
    """
    Gets the checkpoint path given a Sacred dir; defaults to the highest numeric value checkpoint if multiple are found
    :param prev_run: An int or string of the sacred run directory we want to find a checkpoint within
    :return: The full path to a checkpoint within a sacred training dir
    """
    base_path = os.path.join(dir_loc, str(prev_run), 'checkpoints', str(index))
    subdirs = glob(os.path.join(base_path, 'checkpoint_*'))
    if len(subdirs) == 0:
        raise ValueError("No checkpoint subdirs found")
    if len(subdirs) == 1:
        subdir = subdirs[0]
    else:
        subdir = max(subdirs, key=lambda x: _safe_int(x.split('-')[-1]))

    subfiles = [el for el in glob(os.path.join(subdir, 'checkpoint-[0-9]*')) if 'tune_metadata' not in el]
    if len(subfiles) != 1:
        print(subfiles)
        raise ValueError(f"Encountered an unexpected number of checkpoints inside folder {os.path.join(base_path, subdir)}")
    return subfiles[0]

def get_training_policies(trainers):
    """
    Returns the policies that are being actively trained by each trainer
    """
    return [trainers[ind].get_policy(policy_mapping_fn(ind)) for ind in range(len(trainers))]

def get_training_weights(trainers):
    """
    Returns the weights of the policies that are being actively trained by each trainer
    """
    return [trainers[ind].get_weights(policy_mapping_fn(ind))[policy_mapping_fn(ind)] for ind in range(len(trainers))]


def _check_correct_weight_trainability(pretrain_weights, posttrain_weights, ind,
                                       should_not_train='core', should_train='redundant'):
    """
    A testing function, to check whether weights that should be frozen are staying frozen, and weights that should be
    training are changing.
    :param pretrain_weights: The result of deepcopy(get_training_weights(trainers)) before training
    :param posttrain_weights: The result of deepcopy(get_training_weights(trainers)) after training
    :param ind: The index of the policy you want to check
    :param should_not_train: String, `core` or `redundant`, indicating weights that should be frozen. If None, assumes no weights frozen.
    :param should_train: String, `core` or `redundant`, indicating weights that should be trainable. If None, assumes all weights frozen.
    """
    sample_variable = {
        "core": f"{policy_mapping_fn(ind)}/core_move_policy/fc_core_0/kernel",
        "redundant": f"{policy_mapping_fn(ind)}/redundant_move_policy/fc_redundant_0/kernel"
    }
    if should_not_train is not None:
        no_train_before = pretrain_weights[ind][sample_variable[should_not_train]]
        no_train_after = posttrain_weights[ind][sample_variable[should_not_train]]
        assert np.allclose(no_train_before, no_train_after)

    if should_train is not None:
        train_before = pretrain_weights[ind][sample_variable[should_train]]
        train_after = posttrain_weights[ind][sample_variable[should_train]]
        assert not np.allclose(train_before, train_after)



# Iterative Adversarial Training PseudoCode
# Take some (optionally?) pretrained policy P
# Repeat k times:
# --> Fix weights of P
# --> Train adversary A against P, with P unable to update weights, for <some number> of steps. Adversary either from scratch or from pretrain
# --> Fix weights of A
# --> Train P against A, with A unable to update weights, for <some number> of steps

def order_by_agent(victim_index, adv_index, victim_func, adversary_func):
    agent_algorithms = [None, None]
    agent_algorithms[victim_index] = victim_func
    agent_algorithms[adv_index] = adversary_func
    return agent_algorithms


@exploitability_ex.capture
def ordered_policies(victim_index, adv_index, victim_policy, adversary_policy):
    return order_by_agent(victim_index, adv_index, victim_policy, adversary_policy)


@exploitability_ex.capture
def ordered_trainers(victim_index, adv_index, victim_trainer, adversary_trainer):
    return order_by_agent(victim_index, adv_index, victim_trainer, adversary_trainer)


def get_pretrained_weight_subset(trainer, index, pretrained_weight_keywords):
    def any_keyword_in_key(keywords, key):
        for kw in keywords:
            if kw in key:
                return True
        return False

        # Figure out which string key corresponds to the policy in the adversarial position
    assert pretrained_weight_keywords is not None
    policy_key = policy_mapping_fn(index)

    # Pull out initial weights, and remove those we want to re-initialize rather than use pretrained values for
    initial_weights = deepcopy(trainer.get_weights(policy_key))[policy_key]
    initial_weights = {k: v for k, v in initial_weights.items()
                       if any_keyword_in_key(pretrained_weight_keywords, k)}

    return {policy_key: initial_weights}


def possibly_pretrained_trainer(trainer_func, ind, policies, env, logdir, checkpoint_path=None,
                                load_weights=True, pretrained_weight_keywords=None):
    # If load_weights=True pretrained_weight_keywords is None, all weights are kept
    # If load_weights=False, a fully reinitialized trainer is returned
    trainer = get_single_trainer(trainer_func, policies, ind, env, logdir)
    initial_random_weights = trainer.get_weights(policy_mapping_fn(ind))

    if not load_weights:
        # if we're not loading any weights, return a randomly initialized trainer
        return trainer

    trainer.restore(checkpoint_path)
    if pretrained_weight_keywords is None:
        # if we're not subsetting out specific weights to keep, then don't calculate a weight mask
        return trainer

    weight_mask = get_pretrained_weight_subset(trainer, ind, pretrained_weight_keywords)
    trainer.set_weights(initial_random_weights)
    trainer.set_weights(weight_mask)
    return trainer

@exploitability_ex.automain
def my_main(_run, dim, env,prev_run, victim_index, adv_index, outer_loop_iters, adversary_training_iters, victim_finetuning_iters,
            victim_trainer_func, adversary_trainer_func, dir_loc, reload_checkpoint_index):
    ray.init()
    logdir = EXPLOIT_SAVE_DIR + "/" + str(_run._id) + "/rllib/"
    obs_space, act_space, env = get_env(env, dim)
    agent_policies = get_policies(ordered_policies(), obs_space, act_space, dim)
    agent_trainer_functions = ordered_trainers()


    # First, we create the trainers, both of which should have agent1_policy use a frozen model
    trainers = [None, None]

    # This structure uses the global reload_checkpoint_index parameter if it is set, and victim_index if it is not set
    # reload_checkpoint_index is set as None in the default config, meaning each trainer reloads from its respective
    # index, but is overridden in the reload_from_adv_ckpt named config
    trainers[victim_index] = victim_trainer_func(agent_trainer_functions[victim_index], victim_index,
                                                 agent_policies, env, logdir, _get_checkpoint_from_run(index=reload_checkpoint_index if reload_checkpoint_index else victim_index))


    # This framework should allow for doing both exploitability testing and iterative adversarial training in the same framework.
    # If you want to do straight exploitability testing, set victim_finetuning_iters to 0
    # If you want to do iterative adversarial training, set adversary_training_iters and victim_finetuning_iters to the
    # number of steps you want to train the adversary and victim against fixed versions of each other in the inner loop
    # Note that adversary is current reinitialized in the same way in each iteration of the loop

    for t in range(outer_loop_iters):
        print(f"Outer loop iter: {t}/{outer_loop_iters}, {(t/outer_loop_iters)*100}% complete")
        # Create a new trainer object for the adversary
        # TODO allow for an adversary that stays the same across outer_loop iterations
        trainers[adv_index] = adversary_trainer_func(agent_trainer_functions[adv_index], adv_index, agent_policies,
                                                     env, logdir, _get_checkpoint_from_run(index=reload_checkpoint_index if reload_checkpoint_index else adv_index))

        sync_trainers(trainers)
        print("Training adversary")
        for _ in tqdm(range(adversary_training_iters)):
            sync_trainers(trainers)
            trainers[adv_index].train()

        print("Finetuning victim against adversary")
        for _ in tqdm(range(victim_finetuning_iters)):
            sync_trainers(trainers)
            trainers[victim_index].train()

        sync_trainers(trainers)
        for i in range(len(trainers)):
            trainers[i].save(EXPLOIT_SAVE_DIR + "/" + str(_run._id) + "/checkpoints/" + str(i) + "/iter_"+str(t)+"/")
        # TODO calculate performance here and save out in the same directory
        for i in range(len(trainers)):
            trainers[i].save(EXPLOIT_SAVE_DIR + "/" + str(_run._id) + "/checkpoints/" + str(i))
