from sacred import Experiment
from sacred.observers import FileStorageObserver
from marl_training import SAVE_DIR as TRAINING_SAVE_DIR
from measure_exploitability import latest_training_sacred_dir
import json

import matplotlib.pyplot as plt
import pandas as pd

from pathlib import Path
import numpy as np
import os
import seaborn as sns

SAVE_DIR = "graph_generation"

ex = Experiment("graph_generation")
ex.observers.append(FileStorageObserver(SAVE_DIR))

sns.set_style({'font.family': 'Times New Roman'})


def plot_reward_traces(reward_traces_df, colors={"A": 'r', "B": 'g'}):
    print(reward_traces_df.index)
    for k, p in reward_traces_df.index:
        if k == 0:
            reward_traces_df.loc[(k, p), :].plot(color=colors[p], label=str(p))
        else:
            reward_traces_df.loc[(k, p), :].plot(color=colors[p], label='_nolegend_')
    plt.legend()


@ex.capture
def get_config_parameter(desired_param, target_run_id, training_dir):
    config_file = os.path.join(training_dir, target_run_id, "rllib", "params.json")
    with open(config_file) as config_fh:
        params = json.load(config_fh)
    return params[desired_param]


def get_feature_from_results(results_file_name, feature):
    with open(results_file_name) as results_file:
        data = []
        for result_line in results_file:
            result = json.loads(result_line)
            for k in feature:
                result = result[k]

            data.append(result)
    return data

@ex.config
def default_config():
    policy_dir = TRAINING_SAVE_DIR
    # TODO do something more sensible here around policy index
    adversary_ind = 0
    victim_ind = 1
    feature_to_graph = ["custom_metrics", "agent2_policy_total_wins_mean"]
    #feature_to_graph = ["episode_len_mean"]
    image_dir_loc = 'images'
    target_run_id = latest_training_sacred_dir(policy_dir)
    exploitability_policies = ['baseline', 'iat_tot', 'iat_sys', 'iat_strat']
    #exploitability_policies = ['baseline']
    exploitability_adversaries = ['total', 'systemic', 'strategic']
    exploitability_dir_dict = None
    policy_colors = {
        'baseline': 'C9',
        'iat_tot': 'C6'
    }
    adversary_styles = {
        'total': '-',
        'systemic': ':',
        'strategic': '-.'
    }
    dim_styles = {
        10: '-',
        100: ':'
    }
    baseline_opponent_utility=-0.75
    curve_bins=None


def get_latest_sacred_dir_with_param_values(policy_dir, param_values):
    max_val = np.inf
    while True:
        latest_dir = latest_training_sacred_dir(policy_dir, max_val=max_val)
        if latest_dir == "-1":
            break
        with open(os.path.join(policy_dir, latest_dir, 'config.json')) as fp:
            config = json.load(fp)
            param_matches = True
            for param, param_value in param_values.items():
                if config.get(param) != param_value:
                    param_matches = False
                    break

            if param_matches:
                print(f"Found directory for params: {param_values}: {latest_dir}")
                return latest_dir
            else:
                max_val = int(latest_dir)
        if max_val <= 0:
            break
    raise FileNotFoundError(f"No sacred run found with params {param_values}")


def get_adversary_data(results_file_name, feature):
    adversary_list = []
    with open(results_file_name) as results_file:
        adversary = None
        for result_line in results_file:
            #print(result_line)
            result = json.loads(result_line)
            if result["pid"] == 7199: #TODO two jobs wrote to the same file, filtering that out...
                continue
            if int(result['iterations_since_restore']) == 1:
                if adversary is not None:
                    adversary_list.append(adversary)
                adversary = []
            for k in feature:
                result = result[k]
            adversary.append(result)
        adversary_list.append(adversary)
    return np.array(adversary_list)


def get_param_val(policy_dir, run_id, param):
    with open(os.path.join(policy_dir, run_id, 'config.json')) as fp:
        config = json.load(fp)
        return config.get(param)


@ex.capture
def save_image(filename, image_dir_loc):
    image_loc = os.path.join(image_dir_loc, filename)
    Path(image_dir_loc).mkdir(parents=True, exist_ok=True)
    plt.savefig(image_loc)
    ex.add_artifact(image_loc)
    plt.close()


@ex.named_config
def paper_exploitability_curve():
    exploitability_policies = ['baseline']
    exploitability_adversaries = ['total', 'systemic', 'strategic']
    policy_dir="barchart_data"
    curve_bins=50


def get_exploitability_data_from_dir_dict(exploitability_dir_dict, policy_dir):
    exploitability_data = {}
    for experiment_name, run_id in exploitability_dir_dict.items():
        results_file_name = policy_dir + "/" + str(run_id) + "/rllib/result.json"
        adv_index = get_param_val(policy_dir, run_id, "adv_index")
        # TODO make this more general over features, this is a bit hard to do in a way that still allows dynamic adv_index
        adversary_data = get_adversary_data(results_file_name,
                                            ['custom_metrics', f'agent{adv_index + 1}_policy_utility_mean'])
        exploitability_data[experiment_name] = adversary_data
    return exploitability_data


@ex.capture
def get_exploitability_data(policy_dir, exploitability_dir_dict, exploitability_policies, exploitability_adversaries):
    if exploitability_dir_dict is None:
        exploitability_dir_dict = {}
        for policy in exploitability_policies:
            for adversary in exploitability_adversaries:
                param_dict = {"victim_policy_key": policy, "adversary_type_key": adversary}
                sacred_dir = get_latest_sacred_dir_with_param_values(policy_dir, param_dict)
                exploitability_dir_dict[f"{policy}++{adversary}"] = sacred_dir

    return get_exploitability_data_from_dir_dict(exploitability_dir_dict, policy_dir)


# This can also be used for the ablation study test: if we have different policy keys for the policies
# hardened in different ways, this will get picked up and the different policy keys will have different colors
@ex.command
def exploitability_curves(_run, policy_colors, adversary_styles, curve_bins):
    exploitability_data = get_exploitability_data()
    for experiment_name, data_array in exploitability_data.items():
        adversary_average = np.mean(data_array, axis=0)
        adversary_stdev = np.std(data_array, axis=0)
        policy, adversary = experiment_name.split("++")
        if curve_bins is not None:
            ids = np.arange(len(adversary_average)) // curve_bins
            adversary_average = np.bincount(ids,adversary_average)/np.bincount(ids)
        plt.plot(adversary_average, label=experiment_name, color=policy_colors[policy], linestyle=adversary_styles[adversary])
    plt.xlabel("Adversary Training Steps")
    plt.ylabel("Average Adversary Reward")
    plt.legend()
    save_image("exploitability_curves.eps")


# Figure out more clearly what dimensions we want here
# @ex.command
# def exploitability_by_dimension(_run, dim_styles, policy_dir):
#     exploitability_dir_dict = {}
#
#     for dim in dim_styles.keys():
#         param_dict = {"dim": dim}
#         sacred_dir = get_latest_sacred_dir_with_param_values(policy_dir, param_dict)
#         exploitability_dir_dict[f"{policy}_{adversary}"] = sacred_dir


@ex.command
def exploitability_bar_chart(baseline_opponent_utility):
    exploitability_data = get_exploitability_data()
    final_exploitability_values = []
    assert len(set([arr.shape for arr in exploitability_data.values()])) == 1
    for experiment_name, data_array in exploitability_data.items():
        policy, adversary = experiment_name.split('++')
        final_step = [x[-1] for x in data_array]
        #final_step = data_array[:,-1]
        for val in final_step:
            final_exploitability_values.append({'Target Policy Type': policy, 'Adversary Type': adversary, 'Adversary Utility Advantage Over Baseline Opponent': val - baseline_opponent_utility})

    final_values_df = pd.DataFrame(final_exploitability_values)
    sns.barplot(data=final_values_df, x="Target Policy Type", y="Adversary Utility Advantage Over Baseline Opponent", hue="Adversary Type")
    plt.legend(bbox_to_anchor=(.85, 1.15), loc=2, borderaxespad=0.)
    save_image("exploitability_bar_plot.eps")

@ex.command
def episode_length_training_curve(_run, target_run_id, policy_dir, image_dir_loc):

    results_file_name = policy_dir + "/" + str(target_run_id) + "/rllib/result.json"

    feature_to_graph = ["episode_len_mean"]
    data = get_feature_from_results(results_file_name, feature_to_graph)
    plt.plot(data)
    plt.xlabel("Baseline Training Steps")
    plt.ylabel("Average Episode Length")
    save_image("episode_length_curve.eps")



def plot_traces(trace_df):
    trace_df.plot(color='r', label="rest")
    plt.legend()

@ex.automain
def my_main(_run, target_run_id, training_dir, feature_to_graph, image_dir_loc):
    results_file_name = training_dir + "/" + str(target_run_id) + "/rllib/result.json"

    with open(results_file_name) as results_file:
        data = []
        for result_line in results_file:
            result = json.loads(result_line)
            for k in feature_to_graph:
                result = result[k]

            data.append(result)

        data_dic = {"test": data}
        print(data)

    rewards_df = pd.DataFrame(data_dic)

    plot_traces(rewards_df)
    image_loc = image_dir_loc + "/" + str(_run._id) + ".png"

    Path(image_dir_loc).mkdir(parents=True, exist_ok=True)

    plt.savefig(image_loc)
    ex.add_artifact(image_loc)
