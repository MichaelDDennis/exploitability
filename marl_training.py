from sacred import Experiment
from sacred.observers import FileStorageObserver

import ray
from ray.tune.logger import pretty_print

from env_utils import get_env

from alg_utils import get_algs, policy_mapping_fn

SAVE_DIR = "my_runs"

ex = Experiment("marl_training")
ex.observers.append(FileStorageObserver(SAVE_DIR))


def sync_trainers(trainers):
    # Note that this is needlessly quadradic but n=2
    for trainer_a in trainers:
        for i, trainer_b in enumerate(trainers):
            trainer_a.set_weights(trainer_b.get_weights([policy_mapping_fn(i)]))


def run_trainers(trainers):
    for i, trainer in enumerate(trainers):
        print("-- Agent"+str(i)+"--")
        print(pretty_print(trainer.train()))


@ex.config
def my_config():
    agent1_alg = "pg"
    agent2_alg = "pg"
    env = "rps"
    trainning_iters = 1000


@ex.automain
def my_main(_run, agent1_alg, agent2_alg, env, trainning_iters):
    ray.init()
    obs_space, act_space, env = get_env(env)

    trainers, pols = get_algs([agent1_alg, agent2_alg], obs_space, act_space, env)

    for i in range(trainning_iters):
        sync_trainers(trainers)

        run_trainers(trainers)

    sync_trainers(trainers)

    trainers[0].save(SAVE_DIR+"/"+str(_run._id)+"/checkpoints/")
