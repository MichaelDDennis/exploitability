from sacred import Experiment
from sacred.observers import FileStorageObserver

import ray
from ray.tune.logger import pretty_print

from env_utils import get_env

from alg_utils import get_algs, policy_mapping_fn

from tictactoe_wrapper import MultiAgentTicTacToe

from tqdm import tqdm

from sacred import SETTINGS
SETTINGS['CAPTURE_MODE'] = 'sys'

SAVE_DIR = "my_runs"

ex = Experiment("marl_training")
ex.observers.append(FileStorageObserver(SAVE_DIR))


def sync_trainers(trainers):
    # Note that this is needlessly quadradic but n=2
    for j, trainer_a in enumerate(trainers):
        for i, trainer_b in enumerate(trainers):
            trainer_a.set_weights(trainer_b.get_weights([policy_mapping_fn(i)]))


def run_trainers(trainers, silent):
    for i, trainer in enumerate(trainers):
        status = trainer.train()
        if not silent:
            print("-- Agent" + str(i) + "--")
            print(pretty_print(status))


@ex.config
def my_config():
    agent1_alg = "a3c"
    agent2_alg = "a3c"
    env = "tictactoe-v0"
    dim = 2
    trainning_iters = 200
    simulate_trained_pols = True
    silent = True
    gamma = 0.95

@ex.named_config
def freezable_model():
    agent1_alg = "pg_custom"


@ex.capture
def simulate_rollouts_tictactoe(trainers, dim):
    env = MultiAgentTicTacToe(dim)

    total_score = {-1: 0, 0: 0, 1: 0}

    for i in range(20):
        obs = env.reset()

        print("--Rollout"+str(i)+"--")
        done = False
        while not done:
            #print(obs)
            actions = {i: trainer.compute_action(obs[i], policy_id=policy_mapping_fn(i)) for i, trainer in enumerate(trainers)}

            #  idk why this is suddenly necessary TODO figure that out
            actions = {k:[x[0] for x in v] for k, v in actions.items()}

            print(actions)
            obs, rewards, dones, infos = env.step(actions)

            done = dones['__all__']

            if done:
                total_score[rewards[0]] += 1

            env.render()

    print("player 0 outcomes: " + str(total_score))

@ex.automain
def my_main(_run, agent1_alg, agent2_alg, dim, env, trainning_iters, simulate_trained_pols, silent):
    ray.init()
    obs_space, act_space, env_name = get_env(env, dim)

    trainers, pols = get_algs([agent1_alg, agent2_alg], obs_space, act_space, env_name)

    for _ in tqdm(range(trainning_iters)):
        sync_trainers(trainers)

        run_trainers(trainers)
        run_trainers(trainers, silent)

    sync_trainers(trainers)

    trainers[0].save(SAVE_DIR + "/" + str(_run._id) + "/checkpoints/")

    if simulate_trained_pols:
        simulate_rollouts_tictactoe(trainers, dim)


