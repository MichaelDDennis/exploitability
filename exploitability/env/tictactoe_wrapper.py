from ray.rllib.env.multi_agent_env import MultiAgentEnv
import numpy as np
import random
from gym.spaces import Discrete, Tuple, MultiBinary

#TODO Autoregressive Action Distributions
#TODO Current changes (cahnged the observation type, changed the initial state to match the expected observation
# return type, changed the  "played on taken spot" check to look at the first index of the new observation arrays,
# you now just need to keep pushing through this code until all of the errors go away and you return the origonal
# functionality

#TODO Why is the low-level state always all zeros?!?!
#TODO Does the game logic actually work?
#TODO Is the implicit softmax of RLlib messing everything up?

NUM_SQUARES = 9

# First bit says if it is taken, the second says by whom, the rest are noise


def check_game_status(board):  # Taken from gym-tictactoe
    """Return game status by current board status.
    Args:
        board (list): Current board state
    Returns:
        int:
            -1: game in progress
            0: draw game,
            1 or 2 for finished game(winner mark code).
    """
    for t in [1, 2]:
        for j in range(0, 9, 3):
            if [t] * 3 == [board[i] for i in range(j, j+3)]:
                return t
        for j in range(0, 3):
            if board[j] == t and board[j+3] == t and board[j+6] == t:
                return t
        if board[0] == t and board[4] == t and board[8] == t:
            return t
        if board[2] == t and board[4] == t and board[6] == t:
            return t

    for i in range(9):
        if board[i] > 2:
            raise Exception(board)

    for i in range(9):
        if board[i] == 0:
            # still playing
            return -1

    # draw game
    return 0


# Taken from gym-tictactoe
CODE_MARK_MAP = {0: ' ', 1: 'O', 2: 'X'}
LEFT_PAD = '  '


def tomark(code):  # Taken from gym-tictactoe
    return CODE_MARK_MAP[code]


def show_board(board):  # Taken from gym-tictactoe
    """Draw tictactoe board."""
    for j in range(0, 9, 3):

        print(LEFT_PAD + '|'.join([tomark(board[i]) for i in range(j, j + 3)]))
        if j < 6:
            print(LEFT_PAD + '-----')


def high_level(state, k):
    #hls = [(0 if state[i] == 0 else ((state[i]-1)//k)+1) for i in range(NUM_SQUARES)]
    hls = [0 if state[i][0] == 0 else state[i][1]+1 for i in range(NUM_SQUARES)]
    for s in hls:
        if s>2:
            print(k)
            print(state)
            print(hls)
    return hls


def get_other(player):
    return 0 if player == 1 else 1


# Since the code right now expects (2*k + 1) for each cell, we will want to translate that to
# a tuple of [0-3] and [0,1]^k
# alternatively we could use just [0,1] [0,1]^k and use the first of the k bits to represent the player
# alternatively we could use just [0,1] [0,1]^k and use the parity of the string to be the player
# alternatively we could use just [0,1,2]^k and force the first bit to be the player
# We could do any of these randomizing the squares or not
# I kinda want to see if there is a way to have different spaces available

# 5 variables:
# type of tic-tac-toe board
# types of tic-tac-toe moves
# projection of tic-tac-toe board
# projection of tic-tac-toe moves
# Generation of blank moves

# For now we will just break these out into their own functions so that we can easily change things later
# though it might be worth it to eventually make an "meta-game wrapper" for multi-agent
# gym environments, which you could do if you ask for a factorized meta level state s^box = (s,s^box-s)
# which lets the meta-level and mesa-level transition functions to be computed separately.

#Instead of the noise being a one-hot, we can toggle every action, It seems like we can do this just by:
# 0) Boot the code
# 1) Changing the Action/Observation types to match  (try [MultiBinary(2*k+1) for _ in range(9)]) and (Tuple([Discrete(10), MultiBinary(k)]))
# 2) Fix any bugs that ensue
# 3) replacing the softmax on the low-level actions with a tanh.

# TODO We need to make the change here and in the policy definitions


class MultiAgentTicTacToe(MultiAgentEnv):

    def __init__(self, k=1):
        type_list = [MultiBinary(2*k+2) for _ in range(9)]
        #type_list = [Discrete(2 * k + 1) for _ in range(9)]
        type_list.append(Discrete(2))
        self.observation_space = Tuple(type_list)
        action_spaces_list = [Discrete(10)]
        action_spaces_list.extend([Discrete(2) for _ in range(k)])
        self.action_space = Tuple(action_spaces_list)
        #self.action_space = Tuple([Discrete(10), MultiBinary(k)])
        #self.action_space = Tuple([Discrete(10), Discrete(k)])
        self._current_state = [np.array([0 for _ in range(2*k+2)]) for _ in range(NUM_SQUARES)]
        self._current_player = 0
        self._k = k

    def reset(self):
        self._current_state = [np.array([0 for _ in range(2*self._k+2)]) for _ in range(NUM_SQUARES)]
        self._current_player = random.randint(0, 1)
        obs = list(self._current_state)
        obs.append(self._current_player)
        return {0: obs, 1: obs}

    def _get_mover(self):
        return self._current_player

    def _get_non_mover(self):
        return get_other(self._get_mover())

    def _get_end(self, loser):
        obs = list(self._current_state)
        obs.append(self._current_player)
        return {0: obs, 1: obs}, \
               {loser: -1, get_other(loser): 1}, \
               {'__all__': True}, \
               {0: {"broke_rules": loser}, 1: {"broke_rules": loser}}

    def step(self, action_dict):
        if action_dict[self._get_non_mover()][0] != 9:
            return self._get_end(self._get_non_mover())  # played out of turn

        if action_dict[self._get_mover()][0] == 9:
            return self._get_end(self._get_mover())  # no op on own move

        if self._current_state[action_dict[self._get_mover()][0]][0] != 0:
            return self._get_end(self._get_mover())  # played on taken spot

        new_square_state = [1]
        new_square_state.extend([self._current_player])
        if self._current_player == 1:
            new_square_state.extend([0]*self._k)
        new_square_state.extend(action_dict[self._get_mover()][1:])
        if self._current_player == 0:
            new_square_state.extend([0] * self._k)

        self._current_state[action_dict[self._get_mover()][0]] = new_square_state

        # Check if there is a winner
        outcome = check_game_status(high_level(self._current_state, self._k))

        self._current_player = (self._current_player + 1) % 2

        obs = list(self._current_state)
        obs.append(self._current_player)

        if outcome == 0:  # Tie
            return {0: obs, 1: obs}, \
                   {0: 0, 1: 0}, \
                   {'__all__': True}, \
                   {0: {"finished": "tie"}, 1: {"finished": "tie"}}
        elif outcome > 0:  # outcome-1 is winner
            return {0: obs, 1: obs}, \
                   {outcome-1: 1, get_other(outcome-1): -1}, \
                   {'__all__': True}, \
                   {0: {"finished": outcome-1}, 1: {"finished": outcome-1}}

        return {0: obs, 1: obs}, \
               {0: 0, 1: 0}, \
               {'__all__': False}, \
               {0: {}, 1: {}}

    def render(self):
        print(self._current_state)
        show_board(high_level(self._current_state, self._k))
