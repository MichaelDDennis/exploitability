from ray.rllib.agents.pg.pg import PGTrainer, DEFAULT_CONFIG
from ray.rllib.agents.pg.pg_tf_policy import PGTFPolicy, post_process_advantages, pg_tf_loss
from ray.rllib.agents.ppo.ppo import PPOTrainer
from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy
from ray.rllib.agents.a3c.a3c import A3CTrainer
from ray.rllib.agents.a3c.a3c import A3CTFPolicy
from ray.rllib.policy.tf_policy_template import build_tf_policy
from ray.rllib.models import ModelCatalog
from exploitability.utility.freezable_policy import FreezableModularPolicy
from ray.tune.logger import UnifiedLogger

from functools import partial
from copy import deepcopy

from exploitability.env.tictactoe_wrapper import get_other

# We need to register the model with RLLib before we can use it
ModelCatalog.register_custom_model("freezable_modular_policy", FreezableModularPolicy)


def only_train_trainable_variables(policy, optimizer, loss):
    return optimizer.compute_gradients(
        loss, policy.model.trainable_variables())


## Creating a PGTF Policy that respects the Keras model's notion of trainable_variables()
## If we don't do this, even if we build Keras layers as non-trainable, RLLib will still think they're trainable

FreezablePGTFPolicy = build_tf_policy(
    name="FreezablePGTFPolicy",
    get_default_config=lambda: DEFAULT_CONFIG,
    postprocess_fn=post_process_advantages,
    gradients_fn=only_train_trainable_variables,
    loss_fn=pg_tf_loss)


# TODO maybe at some point change the policy names to just be indices
def policy_mapping_fn(agent_id):
    return "agent"+str(agent_id % 2+1)+"_policy"


def utility(episode, policy_string, gamma):
    rewards = episode.agent_rewards[policy_string]
    discount = gamma**episode.length
    return rewards*discount


def on_episode_end(info, gamma):
    episode = info["episode"]
    policy_strings = [policy_mapping_fn(ind) for ind in range(2)]

    for ind, policy_string in enumerate(policy_strings):
        reward = episode._agent_reward_history[ind][-1]
        episode.custom_metrics[policy_string + "_utility"] = reward*(gamma**episode.length)
        episode.custom_metrics[policy_string + "_reward"] = reward

    if policy_strings[0]+"_broke_rules" not in info:
        for policy_string in policy_strings:
            episode.custom_metrics[policy_string + "_broke_rules"] = 0
            episode.custom_metrics[policy_string + "_finished_and_won"] = 0
            episode.custom_metrics[policy_string + "_total_wins"] = 0
            episode.custom_metrics[policy_string + "_total_losses"] = 0
        episode.custom_metrics["ties"] = 0
        episode.custom_metrics["broke_rules"] = 0
        episode.custom_metrics["finished"] = 0

    infos = episode.last_info_for(0)


    if "broke_rules" in infos:
        episode.custom_metrics[policy_mapping_fn(infos["broke_rules"]) + "_broke_rules"] += 1
        episode.custom_metrics[policy_mapping_fn(infos["broke_rules"]) + "_total_losses"] += 1
        episode.custom_metrics[policy_mapping_fn(get_other(infos["broke_rules"])) + "_total_wins"] += 1
        episode.custom_metrics["broke_rules"] += 1

    if "finished" in infos:
        episode.custom_metrics["finished"] += 1

        if infos["finished"] == "tie":
            episode.custom_metrics["ties"] += 1
        else:
            episode.custom_metrics[policy_mapping_fn(infos["finished"]) + "_total_wins"] = + 1
            episode.custom_metrics[policy_mapping_fn(infos["finished"]) + "_finished_and_won"] = + 1
            episode.custom_metrics[policy_mapping_fn(get_other(infos["finished"])) + "_total_losses"] = + 1


def get_ppo_train(name, pols, env, logdir, gamma, shape, lr, batch_size):
    config = {
        "gamma": gamma,
        "sample_batch_size": batch_size,
        "lr": lr,
        "multiagent": {
            "policies": pols,
            "policy_mapping_fn": policy_mapping_fn,
            "policies_to_train": [name],
        },

        "model": {
            "fcnet_activation": "tanh",
            # Number of hidden layers for fully connected net
            "fcnet_hiddens": shape,
        },
        # disable filters, otherwise we would need to synchronize those
        # as well to the DQN agent
        "observation_filter": "NoFilter",
        "callbacks": {
            "on_train_result": on_episode_end
        }}
    return PPOTrainer(env=env,
        config=config, logger_creator=lambda _ : UnifiedLogger(config, logdir))


def get_pg_train(name, pols, env, logdir, gamma, shape, lr, batch_size):
    config = {
            "gamma": gamma,
            "sample_batch_size": batch_size,
            "lr": lr,  #0.01 is too high, 0.0001 is too low , 0.001 seems to work (first 5 are 0.005, last are 0.001)
            "multiagent": {
                "policies": pols,
                "policy_mapping_fn": policy_mapping_fn,
                "policies_to_train": [name],
            },

            "model": {
                "fcnet_activation": "tanh",
                # Number of hidden layers for fully connected net
                "fcnet_hiddens": shape,
            },
            # disable filters, otherwise we would need to synchronize those
            # as well to the DQN agent
            "observation_filter": "NoFilter",
            "callbacks": {
                "on_episode_end": partial(on_episode_end, gamma=gamma)
            }
        }
    return PGTrainer(
        env=env,
        config=config, logger_creator=lambda _ : UnifiedLogger(config, logdir))


def get_a3c_train(name, pols, env, logdir):
    config = {
        "multiagent": {
            "policies": pols,
            "policy_mapping_fn": policy_mapping_fn,
            "policies_to_train": [name],
        },
        # disable filters, otherwise we would need to synchronize those
        # as well to the DQN agent
        "observation_filter": "NoFilter",
        "callbacks": {
            "on_train_result": on_episode_end
        }}
    return A3CTrainer(
        env=env,
        config=config, logger_creator=lambda _: UnifiedLogger(config, logdir))


def get_ppo_pol(obs_space, act_space):
    return PPOTFPolicy, obs_space, act_space, {}


def get_pg_pol(obs_space, act_space, num_redundant_moves=None):
    return PGTFPolicy, obs_space, act_space, {}


def get_hierarchical_pg_pol(obs_space, act_space, num_redundant_moves, redundant_trainable=True, core_trainable=True,
                            core_policy_hiddens=(64,64), redundant_policy_hiddens=(64,64)):

    return FreezablePGTFPolicy, obs_space, act_space, {"model":
                {
                    "custom_model": "freezable_modular_policy",
                    "custom_options": {
                        "redundant_trainable": redundant_trainable,
                        "core_trainable": core_trainable,
                        "num_redundant_moves": num_redundant_moves,
                        "core_policy_hiddens": core_policy_hiddens,
                        "redundant_policy_hiddens": redundant_policy_hiddens
                    }
                }}


def get_a3c_pol(obs_space, act_space):
    return A3CTFPolicy, obs_space, act_space, {}


def get_policies(policies, obs_space, act_space, dim):
    return {policy_mapping_fn(i): policy(obs_space, act_space, dim) for i, policy in enumerate(policies)}


def get_single_trainer(trainer, pols, ind, env, logdir):
    return trainer(policy_mapping_fn(ind), pols, env, logdir)


def get_pretrained_weight_subset(trainer, index, pretrained_weight_keywords):
    def any_keyword_in_key(keywords, key):
        for kw in keywords:
            if kw in key:
                return True
        return False

    assert pretrained_weight_keywords is not None
    policy_key = policy_mapping_fn(index)

    # Pull out initial weights, and remove those we want to re-initialize rather than use pretrained values for
    initial_weights = deepcopy(trainer.get_weights(policy_key))[policy_key]
    initial_weights = {k: v for k, v in initial_weights.items()
                       if any_keyword_in_key(pretrained_weight_keywords, k)}

    return {policy_key: initial_weights}


def possibly_pretrained_trainer(trainer_func, ind, policies, env, logdir, checkpoint_path=None,
                                load_weights=True, pretrained_weight_keywords=None):
    # If load_weights=True pretrained_weight_keywords is None, all weights are kept
    # If load_weights=False, a fully reinitialized trainer is returned
    trainer = get_single_trainer(trainer_func, policies, ind, env, logdir)
    initial_random_weights = trainer.get_weights(policy_mapping_fn(ind))

    if not load_weights:
        # if we're not loading any weights, return a randomly initialized trainer
        return trainer

    trainer.restore(checkpoint_path)
    if pretrained_weight_keywords is None:
        # if we're not subsetting out specific weights to keep, then don't calculate a weight mask
        return trainer

    weight_mask = get_pretrained_weight_subset(trainer, ind, pretrained_weight_keywords)
    trainer.set_weights(initial_random_weights)
    trainer.set_weights(weight_mask)
    return trainer