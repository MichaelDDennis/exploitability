from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np

from ray.rllib.models.tf.tf_modelv2 import TFModelV2
from ray.rllib.models.tf.misc import normc_initializer, get_activation_fn
from ray.rllib.utils import try_import_tf
import pdb
tf = try_import_tf()

# based on rllib.models.tf.fcnet_v2.py


def redundant_to_core(space_vector, num_redundant_moves):
    ind = tf.argmax(space_vector, axis=1)
    # if ind/num_redundant_moves < 1, rounds down to 0 for player 0. If between 1 and 2, rounds down to 1. Else 2
    player = tf.floordiv(ind, num_redundant_moves)
    #player = space_vector[1]
    return player


def get_core_game_state(tns, num_redundant_moves, num_game_spaces=9):
    game_info = tns[:, 0:-2]
    # The last two elements of the full game state are two indicator variables for whether it's each player's turn
    turn_info = tns[:, -2:]
    # Turn info starts as a binary where (hopefully) the first element being 1 means its player zero's turn
    # and the second element being 1 means it's player one's turn. This means we can just directly use an argmax
    turn_info = tf.cast(tf.argmax(turn_info, axis=1), tf.float32)
    # In the redundant game state, moves are represented as one of 2*num_redundant_moves +1 (whose chip) + 1 (empty) at each space,
    # and we want to instead represent them a one-hot of 3 options (X, O, or empty)
    per_space_vectors = tf.split(game_info, [2*num_redundant_moves+2]*num_game_spaces, axis=1)
    per_space_core_state = [tf.cast(redundant_to_core(space_vector, num_redundant_moves), tf.float32) for space_vector in per_space_vectors]
    per_space_core_state.append(turn_info)
    core_tensor_state = tf.transpose(tf.stack(per_space_core_state))
    return core_tensor_state


class FreezableModularPolicy(TFModelV2):
    """
    A policy that extends the TFModelV2 class (which is responsible for holding the internal tensorflow logic of policies)
    to implement a new policy particular to this experimental framework. Relevant properties of this policy:
    - It's designed for an environment where players are playing Tic Tac Toe with multiple redundant low-level moves
    - It has separate components of the network that are responsible for predicting the core and redundant components of the
    move, and those components can be separately set to trainable = True/False
    """
    def __init__(self, obs_space, action_space, num_outputs, model_config,
                 name, verbose=False):
        super(FreezableModularPolicy, self).__init__(
            obs_space, action_space, num_outputs, model_config, name)
        custom_model_options = model_config.get("custom_options")
        train_core_policy = custom_model_options.get("core_trainable")
        train_redundant_policy = custom_model_options.get("redundant_trainable")
        num_redundant_moves = custom_model_options.get("num_redundant_moves")
        num_game_spaces = custom_model_options.get("num_game_spaces", 9)
        core_policy_hiddens = custom_model_options.get("core_policy_hiddens")
        redundant_policy_hiddens = custom_model_options.get("redundant_policy_hiddens")
        activation = "tanh"

        inputs = tf.keras.layers.Input(
            shape=(np.product(obs_space.shape),), name="observations")
        # Take in the state of the game board, including variations in redundant moves, and
        # create a core game state which only contains strategically-relevant information (i.e. whose turn it is,
        # where Xs and Os are placed) and ignores redundant state information.
        game_state = get_core_game_state(inputs, num_redundant_moves, num_game_spaces)
        #verbose=True
        if verbose:
            game_state = tf.keras.layers.Lambda(
                (lambda x: tf.Print(x, [x], message="game_state", first_n=-1, summarize=1024)),
                name="game_state")(game_state)

        # Create a core policy: producing a softmax over core moves given core game state
        last_layer = game_state
        with tf.variable_scope("core_move_policy"):
            for layer_ind, size in enumerate(core_policy_hiddens):
                last_layer = tf.keras.layers.Dense(
                    size,
                    name=f"fc_core_{layer_ind}",
                    activation=activation,
                    bias_initializer='ones',  #have to add this or the inital policy will understand that it shouldn't move when it's not it's turn,
                    trainable=train_core_policy)(last_layer)

            core_move = tf.keras.layers.Dense(
                    num_game_spaces + 1,
                    name="fc_core_move_softmax",
                    activation=None,
                    bias_initializer='ones',
                    trainable=train_core_policy)(last_layer)
        if verbose:
            core_move = tf.keras.layers.Lambda((lambda x: tf.Print(x, [x], message="core_move", first_n=-1, summarize=1024)),
                                            name="core_move")(core_move)

        # Create an input consisting of the core move softmax from the previous module, appended to the original inputs,
        # which contain redundant move information. This info is passed into a policy which predicts a softmax over
        # redundant moves. Full move is sampled by first sampling a core action, then independently sampling a redundant action
        merged_input = tf.concat([core_move, inputs], axis=1)
        last_layer = merged_input
        with tf.variable_scope("redundant_move_policy"):
            for layer_ind, size in enumerate(redundant_policy_hiddens):
                last_layer = tf.keras.layers.Dense(
                    size,
                    name=f"fc_redundant_{layer_ind}",
                    activation=activation,
                    bias_initializer='ones',
                    trainable=train_redundant_policy)(last_layer)

            redundant_move = tf.keras.layers.Dense(
                         num_redundant_moves*2,
                         name="fc_redundant_move_softmax",
                         activation=None,
                         bias_initializer='ones',
                         kernel_initializer=normc_initializer(1.0),
                         trainable=train_core_policy)(last_layer)
        if verbose:
            redundant_move = tf.keras.layers.Lambda((lambda x: tf.Print(x, [x], message="redundant_move", first_n=-1, summarize=1024)),
                                            name="redundant_move")(redundant_move)

        # Return an output that is a concatenation of <softmax over core move> and <softmax over redundant move>
        output = tf.concat([core_move, redundant_move], axis=1)

        if verbose:
            output = tf.keras.layers.Lambda((lambda x: tf.Print(x, [x], message="output", first_n=-1, summarize=1024)), name="output")(output)

        self.base_model = tf.keras.Model(inputs, output)
        self.register_variables(self.base_model.variables)

    def trainable_variables(self):
        # Getting around a tensorflow bug where Keras' base_model.trainable_variables isn't represented in the trainable
        # flags on the underlying variables.
        # See: https://github.com/tensorflow/tensorflow/issues/36349
        return self.base_model.trainable_variables

    def forward(self, input_dict, state, seq_lens):
        model_out = self.base_model(input_dict["obs_flat"])
        return model_out, state
