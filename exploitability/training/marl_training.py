from sacred import Experiment
from sacred.observers import FileStorageObserver

import ray
from ray.tune.logger import pretty_print

from exploitability.utility.env_utils import get_env

from exploitability.utility.alg_utils import policy_mapping_fn, get_pg_pol, get_pg_train, get_hierarchical_pg_pol, get_policies, get_single_trainer

from exploitability.env.tictactoe_wrapper import MultiAgentTicTacToe

from tqdm import tqdm

from sacred import SETTINGS
from functools import partial


SETTINGS['CAPTURE_MODE'] = 'sys'

SAVE_DIR = "baseline_training"

ex = Experiment("marl_training")
ex.observers.append(FileStorageObserver(SAVE_DIR))


def sync_trainers(trainers):
    # Note that this is needlessly quadradic but n=2
    for j, trainer_a in enumerate(trainers):
        for i, trainer_b in enumerate(trainers):
            trainer_a.set_weights(trainer_b.get_weights([policy_mapping_fn(i)]))


def run_trainers(trainers, silent):
    for i, trainer in enumerate(trainers):
        status = trainer.train()
        if not silent:
            print("-- Agent" + str(i) + "--")
            print(pretty_print(status))


@ex.config
def my_config():
    gamma = 0.95
    agent1_policy = get_pg_pol
    agent2_policy = get_pg_pol
    agent1_trainer = partial(get_pg_train, gamma=gamma)
    agent2_trainer = partial(get_pg_train, gamma=gamma)
    env = "tictactoe-v0"
    dim = 2
    training_iters = 200
    simulate_trained_pols = True
    silent = True
    checkpoint_freq = 50

@ex.named_config
def freezable_model():
    agent1_policy = get_hierarchical_pg_pol
    agent2_policy = get_hierarchical_pg_pol
    training_iters = 3
    simulate_trained_pols = False
    silent = True


@ex.named_config
def paper_config_baseline_train():
    env = "tictactoe-v0"
    dim = 10
    silent = True
    gamma = 0.95
    checkpoint_freq = 50
    training_iters = 5000
    simulate_trained_pols = True
    agent1_policy = get_pg_pol
    agent2_policy = partial(get_hierarchical_pg_pol, core_policy_hiddens=(256,), redundant_policy_hiddens=(256,))

@ex.capture
def simulate_rollouts_tictactoe(trainers, dim):
    env = MultiAgentTicTacToe(dim)

    total_score = {-1: 0, 0: 0, 1: 0}

    for i in range(20):
        obs = env.reset()

        print("--Rollout"+str(i)+"--")
        print("First Player: " + str(obs[0][9]))
        done = False
        while not done:
            #print(obs)
            actions = {i: trainer.compute_action(obs[i], policy_id=policy_mapping_fn(i)) for i, trainer in enumerate(trainers)}

            #  idk why this is suddenly necessary TODO figure that out
            actions = {k:[x[0] for x in v] for k, v in actions.items()}

            print(actions)
            obs, rewards, dones, infos = env.step(actions)

            done = dones['__all__']

            if done:
                total_score[rewards[0]] += 1

            env.render()

    print("player 0 outcomes: " + str(total_score))

@ex.automain
def my_main(_run, agent1_policy, agent2_policy, agent1_trainer, agent2_trainer, dim, env, training_iters, simulate_trained_pols, silent, checkpoint_freq):
    ray.init()
    obs_space, act_space, env_name = get_env(env, dim)

    logdir = SAVE_DIR + "/" + str(_run._id) + "/rllib/"

    pols = get_policies([agent1_policy, agent2_policy], obs_space, act_space, dim)
    trainers = [get_single_trainer(trainer, pols, ind, env, logdir) for ind, trainer in enumerate([agent1_trainer, agent2_trainer])]

    for t in tqdm(range(training_iters)):
        sync_trainers(trainers)

        if t % checkpoint_freq == 0:
            for i in range(len(trainers)):
                trainers[i].save(SAVE_DIR + "/" + str(_run._id) + "/checkpoints/"+str(i)+"/")

        run_trainers(trainers, silent)

    sync_trainers(trainers)

    for i in range(len(trainers)):
        trainers[i].save(SAVE_DIR + "/" + str(_run._id) + "/checkpoints/"+str(i)+"/")

    print(trainers[0].collect_metrics())

    if simulate_trained_pols:
        simulate_rollouts_tictactoe(trainers, dim)


