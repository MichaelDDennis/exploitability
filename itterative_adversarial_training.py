from sacred import Experiment
from sacred.observers import FileStorageObserver

from tqdm import tqdm

import ray
from ray.tune.logger import pretty_print

from env_utils import get_env

from alg_utils import get_algs, get_new_train

from marl_training import sync_trainers

SAVE_DIR = "my_runs"

itterative_adverarial_training_ex = Experiment("itterative_adverarial_training")
itterative_adverarial_training_ex.observers.append(FileStorageObserver(SAVE_DIR))

@itterative_adverarial_training_ex.config
def my_config():
    agent1_alg = "pg"
    agent2_alg = "pg"
    env = "rps"
    adv_trainning_iters = 10
    trainning_iters = 1000

VIC_INDEX=0
ADV_INDEX=1


@itterative_adverarial_training_ex.automain
def my_main(_run, agent1_alg, agent2_alg, env, adv_trainning_iters, trainning_iters):
    ray.init()
    obs_space, act_space, env = get_env(env)

    trainers, pols = get_algs([agent1_alg, agent2_alg], obs_space, act_space, env)

    for t in range(trainning_iters):
        adv = get_new_train(agent2_alg, env, pols, 1)
        trainers[ADV_INDEX]=adv
        sync_trainers(trainers)
        print("before")
        print(pretty_print(adv.train()))
        for i in tqdm(range(adv_trainning_iters)):
            sync_trainers(trainers)

            adv.train()
        print("after")
        print(pretty_print(adv.train()))
        trainers[VIC_INDEX].train()
        sync_trainers(trainers)

    sync_trainers(trainers)

    trainers[1].save(SAVE_DIR+"/"+str(_run._id)+"/checkpoints/")
