from ray.rllib.env.multi_agent_env import MultiAgentEnv

from gym.spaces import Discrete, Tuple


NUM_SQUARES = 9


def check_game_status(board):  # Taken from gym-tictactoe
    """Return game status by current board status.
    Args:
        board (list): Current board state
    Returns:
        int:
            -1: game in progress
            0: draw game,
            1 or 2 for finished game(winner mark code).
    """
    for t in [1, 2]:
        for j in range(0, 9, 3):
            if [t] * 3 == [board[i] for i in range(j, j+3)]:
                return t
        for j in range(0, 3):
            if board[j] == t and board[j+3] == t and board[j+6] == t:
                return t
        if board[0] == t and board[4] == t and board[8] == t:
            return t
        if board[2] == t and board[4] == t and board[6] == t:
            return t

    for i in range(9):
        if board[i] > 2:
            raise Exception(board)

    for i in range(9):
        if board[i] == 0:
            # still playing
            return -1

    # draw game
    return 0


# Taken from gym-tictactoe
CODE_MARK_MAP = {0: ' ', 1: 'O', 2: 'X'}
LEFT_PAD = '  '


def tomark(code):  # Taken from gym-tictactoe
    return CODE_MARK_MAP[code]


def show_board(board):  # Taken from gym-tictactoe
    """Draw tictactoe board."""
    for j in range(0, 9, 3):

        print(LEFT_PAD + '|'.join([tomark(board[i]) for i in range(j, j + 3)]))
        if j < 6:
            print(LEFT_PAD + '-----')


def high_level(state, k):
    hls = [(0 if state[i] == 0 else ((state[i]-1)//k)+1) for i in range(NUM_SQUARES)]
    return hls

class MultiAgentTicTacToe(MultiAgentEnv):

    def __init__(self, k=1):
        self.observation_space = Tuple([Discrete(2*k+1) for _ in range(10)])
        self.action_space = Tuple([Discrete(10), Discrete(k)])
        self._current_state = [0 for _ in range(NUM_SQUARES)]
        self._current_player = 0
        self._k = k

    def reset(self):
        self._current_state = [0 for _ in range(NUM_SQUARES)]
        self._current_player = 0
        obs = list(self._current_state)
        obs.append(self._current_player)
        return {0: obs, 1: obs}

    def _get_mover(self):
        return self._current_player

    def _get_other(self, player):
        return 0 if player == 1 else 1

    def _get_non_mover(self):
        return self._get_other(self._get_mover())

    def _get_end(self, loser):
        obs = list(self._current_state)
        obs.append(self._current_player)
        return {0: obs, 1: obs}, \
               {loser: -1, self._get_other(loser): 1}, \
               {'__all__': True}, \
               {0: {}, 1: {}}

    def step(self, action_dict):
        if action_dict[self._get_non_mover()][0] != 9:
            return self._get_end(self._get_non_mover())  # played out of turn

        if action_dict[self._get_mover()][0] == 9:
            return self._get_end(self._get_mover())  # no op on own move

        if self._current_state[action_dict[self._get_mover()][0]] != 0:
            return self._get_end(self._get_mover())  # played on taken spot

        self._current_state[action_dict[self._get_mover()][0]] = (self._current_player*self._k)+1+action_dict[self._get_mover()][1]

        # Check if there is a winner
        outcome = check_game_status(high_level(self._current_state, self._k))

        self._current_player = (self._current_player + 1) % 2

        obs = list(self._current_state)
        obs.append(self._current_player)

        if outcome == 0:  # Tie
            return {0: obs, 1: obs}, \
                   {0: 0, 1: 0}, \
                   {'__all__': True}, \
                   {0: {}, 1: {}}
        elif outcome > 0:  # outcome-1 is winner
            return {0: obs, 1: obs}, \
                   {outcome-1: 1, self._get_other(outcome-1): -1}, \
                   {'__all__': True}, \
                   {0: {}, 1: {}}

        return {0: obs, 1: obs}, \
               {0: 0, 1: 0}, \
               {'__all__': False}, \
               {0: {}, 1: {}}

    def render(self):
        show_board(high_level(self._current_state, self._k))

