from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np

from ray.rllib.models.tf.tf_modelv2 import TFModelV2
from ray.rllib.models.tf.misc import normc_initializer, get_activation_fn
from ray.rllib.utils import try_import_tf
import pdb
tf = try_import_tf()

# based on rllib.models.tf.fcnet_v2.py

def redundant_to_core(space_vector, num_redundant_moves):
    ind = tf.argmax(space_vector, axis=1)
    # if ind/num_redundant_moves < 1, rounds down to 0 for player 0. If between 1 and 2, rounds down to 1. Else 2
    player = tf.floordiv(ind, num_redundant_moves)
    return player

def get_core_game_state(tns, num_redundant_moves, num_game_spaces=9):
    # Takes in: one-hot representation of redundant game state
    # Returns: a discrete (maybe someday one-hot) representation of game state -> 9 integers for game slot and then 1 for turn

    # TODO figure out how to do this slicing properly with batch dimensions

    game_info = tns[:, 0:-2]
    turn_info = tns[:, -2:]
    # turn info starts as a binary where (hopefully) the first element being 1 means its player zero's turn
    # and the second element being 1 means it's player one's turn. This means we can just directly use an argmax
    turn_info = tf.cast(tf.argmax(turn_info, axis=1), tf.float32)
    per_space_vectors = tf.split(game_info, [2*num_redundant_moves+1]*num_game_spaces, axis=1)
    per_space_core_state = [tf.cast(redundant_to_core(space_vector, num_redundant_moves), tf.float32) for space_vector in per_space_vectors]
    per_space_core_state.append(turn_info)
    core_tensor_state = tf.transpose(tf.stack(per_space_core_state))
    return core_tensor_state


# TODO figure out what value num_outputs will take and whether we should
# just ignore it
# we are using obs_flat, so take the flattened shape as input

# Okay, so the input here will be [one-hot-vector of length 2k+1] repeated for each game slot
# in particular, obs_space.shape will be a tuple of shapes
# We then want to create a transformation of that layer that represents "core game state"
# which we can get by:
# splitting the vector up into [game info] and [turn info]
# within [game info], split into chunks of 2K+1
# within each chunk, there will (hopefully) only be one value set to 1. Get the index of it. If the index
# is at position -1, the spot is empty. If it's < k, it's player 1 mark, if it's >=k but < -1, it's player
# 2 mark


# Result -> vector of length 3 (?is the onehot encoding using all indices?)*9 + a single bit for turn
# this we'll call core_state

# Create some module that takes in core state and produces a softmax(10) over mutually exclusive core actions.
# Each layer in this module has a trainable parameter set to the value of train_core_policy

# This softmax output we'll call core_actions

# Next, we'll create a module that takes in (inputs) and also core_actions, concatenated together.

# This produces a softmax(k)

class FreezableModularPolicy(TFModelV2):

    def __init__(self, obs_space, action_space, num_outputs, model_config,
                 name):
        super(FreezableModularPolicy, self).__init__(
            obs_space, action_space, num_outputs, model_config, name)
        custom_model_options = model_config.get("custom_options")
        train_core_policy = custom_model_options.get("core_trainable")
        train_redundant_policy = custom_model_options.get("redundant_trainable")
        num_redundant_moves = custom_model_options.get("num_redundant_moves")
        num_game_spaces = custom_model_options.get("num_game_spaces", 9)
        core_policy_hiddens = custom_model_options.get("core_policy_hiddens")
        redundant_policy_hiddens = custom_model_options.get("redundant_policy_hiddens")
        activation = "tanh"
        inputs = tf.keras.layers.Input(
            shape=(np.product(obs_space.shape),), name="observations")
        game_state = get_core_game_state(inputs, num_redundant_moves, num_game_spaces)
        last_layer = game_state
        with tf.variable_scope("core_move_policy"):
            for layer_ind, size in enumerate(core_policy_hiddens):
                last_layer = tf.keras.layers.Dense(
                    size,
                    name=f"fc_core_{layer_ind}",
                    activation=activation,
                    trainable=train_core_policy)(last_layer)

            core_move = tf.keras.layers.Dense(
                    num_game_spaces + 1,
                    name="fc_core_move_softmax",
                    activation="softmax",
                    trainable=train_core_policy)(last_layer)
        merged_input = tf.concat([core_move, inputs], axis=1)
        last_layer = merged_input
        with tf.variable_scope("redundant_move_policy"):
            for layer_ind, size in enumerate(redundant_policy_hiddens):
                last_layer = tf.keras.layers.Dense(
                    size,
                    name=f"fc_redundant_{layer_ind}",
                    activation=activation,
                    trainable=train_redundant_policy)(last_layer)

            redundant_move = tf.keras.layers.Dense(
                         num_redundant_moves,
                        name="fc_redundant_move_softmax",
                        activation="softmax",
                        kernel_initializer=normc_initializer(1.0),
                        trainable=train_core_policy)(last_layer)
        output = tf.concat([core_move, redundant_move], axis=1)
        self.base_model = tf.keras.Model(inputs, output)
        self.register_variables(self.base_model.variables)

    def trainable_variables(self):
        return self.base_model.trainable_variables

    def forward(self, input_dict, state, seq_lens):
        model_out = self.base_model(input_dict["obs_flat"])
        return model_out, state
