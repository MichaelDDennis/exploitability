from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np

from ray.rllib.models.tf.tf_modelv2 import TFModelV2
from ray.rllib.models.tf.misc import normc_initializer, get_activation_fn
from ray.rllib.utils import try_import_tf

tf = try_import_tf()


class FreezableModularPolicy(TFModelV2):
    """ ."""

    def __init__(self, obs_space, action_space, num_outputs, model_config,
                 name):
        super(FreezableModularPolicy, self).__init__(
            obs_space, action_space, num_outputs, model_config, name)
        train_core_policy = model_config.get("core_trainable")
        train_redundant_policy = model_config.get("redundant_trainable")
        num_redundant_moves = model_config.get("num_redundant_moves")
        activation = "relu"

        # TODO figure out what value num_outputs will take and whether we should
        # just ignore it
        # we are using obs_flat, so take the flattened shape as input

        # Okay, so the input here will be [one-hot-vector of length 2k+1] repeated for each game slot
        # in particular, obs_space.shape will be a tuple of shapes
        inputs = tf.keras.layers.Input(
            shape=(2*, ), name="observations")

        # We then want to create a transformation of that layer that represents "core game state"
        # which we can get by:
        # splitting the vector up into [game info] and [turn info]
        # within [game info], split into chunks of 2K+1
        # within each chunk, there will (hopefully) only be one value set to 1. Get the index of it. If the index
        # is at position -1, the spot is empty. If it's < k, it's player 1 mark, if it's >=k but < -1, it's player
        # 2 mark


        # Result -> vector of length 3 (?is the onehot encoding using all indices?)*9 + a single bit for turn
        # this we'll call core_state

        # Create some module that takes in core state and produces a softmax(10) over mutually exclusive core actions.
        # Each layer in this module has a trainable parameter set to the value of train_core_policy

        # This softmax output we'll call core_actions

        # Next, we'll create a module that takes in (inputs) and also core_actions, concatenated together.

        # This produces a softmax(k)


        layer_out = tf.keras.layers.Dense(
            num_outputs,
            name="fc_out",
            activation=None,
            kernel_initializer=normc_initializer(0.01))(last_layer)


        self.base_model = tf.keras.Model(inputs, [layer_out, value_out])
        self.register_variables(self.base_model.variables)

    def forward(self, input_dict, state, seq_lens):
        model_out,  = self.base_model(input_dict["obs_flat"])
        return model_out, state

    def value_function(self):
        return tf.reshape(self._value_out, [-1])
