from sacred import Experiment
from marl_training import policy_mapping_fn, sync_trainers, simulate_rollouts_tictactoe
from env_utils import get_env
import ray
from alg_utils import get_pg_pol, get_pg_train, get_policies, possibly_pretrained_trainer, get_hierarchical_pg_pol
from measure_exploitability import _get_checkpoint_from_run
from functools import partial


ex = Experiment("simulate_rollouts")

@ex.config
def my_config():
    gamma = 0.95
    prev_run = None
    dir_loc = None
    agent_algorithms = [get_pg_pol,  partial(get_hierarchical_pg_pol,
                                             core_policy_hiddens=(256,),
                                             redundant_policy_hiddens=(256,))]
    agent_trainers = [partial(get_pg_train, gamma=gamma), partial(get_pg_train, gamma=gamma)]
    env = "tictactoe-v0"
    dim = 10
    training_iters = 200
    simulate_trained_pols = True
    silent = True
    checkpoint_freq = 50

@ex.automain
def my_main(_run, agent_algorithms, agent_trainers, dim, env, simulate_trained_pols, prev_run, dir_loc):
    ray.init()
    obs_space, act_space, env_name = get_env(env, dim)
    policies = get_policies(agent_algorithms, obs_space, act_space, dim)
    checkpoint_paths = [_get_checkpoint_from_run(ind, prev_run, dir_loc) for ind in range(2)]
    logdir = "~/raydir/" # change this
    trainers = [
        possibly_pretrained_trainer(agent_trainers[ind], ind, policies, env, logdir, checkpoint_paths[ind])
        for ind in range(2)
    ]
    sync_trainers(trainers)

    if simulate_trained_pols:
        simulate_rollouts_tictactoe(trainers, dim)
