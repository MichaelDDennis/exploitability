from ray.rllib.agents.pg.pg import PGTrainer, DEFAULT_CONFIG
from ray.rllib.agents.pg.pg_policy import PGTFPolicy, postprocess_advantages, policy_gradient_loss
from ray.rllib.agents.ppo.ppo import PPOTrainer
from ray.rllib.agents.ppo.ppo_policy import PPOTFPolicy
from ray.rllib.agents.a3c.a3c import A3CTrainer
from ray.rllib.agents.a3c.a3c import A3CTFPolicy
from ray.rllib.policy.tf_policy_template import build_tf_policy
from ray.rllib.models import ModelCatalog
from freezable_policy import FreezableModularPolicy

from functools import partial

# We need to register the model with RLLib before we can use it
ModelCatalog.register_custom_model("freezable_modular_policy", FreezableModularPolicy)


def only_train_trainable_variables(policy, optimizer, loss):
    return optimizer.compute_gradients(
        loss, policy.model.trainable_variables())

## Creating a PGTF Policy that respects the Keras model's notion of trainable_variables()
FreezablePGTFPolicy = build_tf_policy(
    name="FreezablePGTFPolicy",
    get_default_config=lambda: DEFAULT_CONFIG,
    postprocess_fn=postprocess_advantages,
    gradients_fn=only_train_trainable_variables,
    loss_fn=policy_gradient_loss)

# TODO maybe at some point change the policy names to just be indices
def policy_mapping_fn(agent_id):
    return "agent"+str(agent_id % 2+1)+"_policy"


def get_ppo_train(name, pols, env):
    return PPOTrainer(
        env=env,
        config={
            "multiagent": {
                "policies": pols,
                "policy_mapping_fn": policy_mapping_fn,
                "policies_to_train": [name],
            },
            # disable filters, otherwise we would need to synchronize those
            # as well to the DQN agent
            "observation_filter": "NoFilter",
        })


def get_pg_train(name, pols, env):
    return PGTrainer(
        env=env,
        config={
            "gamma": 0.95,
            "sample_batch_size": 2000,
            "multiagent": {
                "policies": pols,
                "policy_mapping_fn": policy_mapping_fn,
                "policies_to_train": [name],
            },
            # disable filters, otherwise we would need to synchronize those
            # as well to the DQN agent
            "observation_filter": "NoFilter",
        })

def get_a3c_train(name, pols, env):
    return A3CTrainer(
        env=env,
        config={
            "multiagent": {
                "policies": pols,
                "policy_mapping_fn": policy_mapping_fn,
                "policies_to_train": [name],
            },
            # disable filters, otherwise we would need to synchronize those
            # as well to the DQN agent
            "observation_filter": "NoFilter",
        })


def get_ppo_pol(obs_space, act_space):
    return PPOTFPolicy, obs_space, act_space, {}


def get_pg_pol(obs_space, act_space):
    return PGTFPolicy, obs_space, act_space, {}


def get_hierarchical_pg_pol(obs_space, act_space, redundant_trainable=True, core_trainable=True, num_redundant_moves=2,
                            core_policy_hiddens=(50, 25,), redundant_policy_hiddens=(100, 25,)):

    return FreezablePGTFPolicy, obs_space, act_space, {"model":
                {
                    "custom_model": "freezable_modular_policy",
                    "custom_options": {
                        "redundant_trainable": redundant_trainable,
                        "core_trainable": core_trainable,
                        "num_redundant_moves": num_redundant_moves,
                        "core_policy_hiddens": core_policy_hiddens,
                        "redundant_policy_hiddens": redundant_policy_hiddens
                    }
                }}


def get_a3c_pol(obs_space, act_space):
    return A3CTFPolicy, obs_space, act_space, {}


ALG_POL_MAP = {"pg": get_pg_pol, "ppo": get_ppo_pol, "pg_hierarchical": get_hierarchical_pg_pol, "a3c": get_a3c_pol,
               "pg_hierarchical_core_frozen": partial(get_hierarchical_pg_pol, core_trainable=False),
               "pg_hierarchical_redundant_frozen": partial(get_hierarchical_pg_pol, redundant_trainable=False)}

ALG_TRAIN_MAP = {"pg": get_pg_train, "ppo": get_ppo_train, "pg_hierarchical": get_pg_train, "a3c": get_a3c_train,
                 "pg_hierarchical_core_frozen": get_pg_train, "pg_hierarchical_redundant_frozen": get_pg_train}


def get_policies(algs, obs_space, act_space):
    return {policy_mapping_fn(i): ALG_POL_MAP[alg](obs_space, act_space) for i, alg in enumerate(algs)}


def get_single_trainer(alg, pols, ind, env):
    return ALG_TRAIN_MAP[alg](policy_mapping_fn(ind), pols, env)


def get_algs(algs, obs_space, act_space, env):
    pols = get_policies(algs, obs_space, act_space)
    return [get_single_trainer(alg, pols, i, env) for i, alg in enumerate(algs)], pols
