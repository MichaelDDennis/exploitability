from ray.rllib.agents.pg.pg import PGTrainer, DEFAULT_CONFIG
from ray.rllib.agents.pg.pg_policy import PGTFPolicy, postprocess_advantages, policy_gradient_loss
from ray.rllib.agents.ppo.ppo import PPOTrainer
from ray.rllib.agents.ppo.ppo_policy import PPOTFPolicy
from ray.rllib.agents.a3c.a3c import A3CTrainer
from ray.rllib.agents.a3c.a3c import A3CTFPolicy
from ray.rllib.policy.tf_policy_template import build_tf_policy
from ray.rllib.models import ModelCatalog
from freezable_policy import FreezableModularPolicy
from ray.tune.logger import UnifiedLogger

from functools import partial

from tictactoe_wrapper import get_other

# We need to register the model with RLLib before we can use it
ModelCatalog.register_custom_model("freezable_modular_policy", FreezableModularPolicy)


def only_train_trainable_variables(policy, optimizer, loss):
    return optimizer.compute_gradients(
        loss, policy.model.trainable_variables())


## Creating a PGTF Policy that respects the Keras model's notion of trainable_variables()
FreezablePGTFPolicy = build_tf_policy(
    name="FreezablePGTFPolicy",
    get_default_config=lambda: DEFAULT_CONFIG,
    postprocess_fn=postprocess_advantages,
    gradients_fn=only_train_trainable_variables,
    loss_fn=policy_gradient_loss)


# TODO maybe at some point change the policy names to just be indices
def policy_mapping_fn(agent_id):
    return "agent"+str(agent_id % 2+1)+"_policy"


def on_episode_end(info):
    episode = info["episode"]
    if policy_mapping_fn(0)+"_broke_rules" not in info:
        episode.custom_metrics[policy_mapping_fn(0) + "_broke_rules"] = 0
        episode.custom_metrics[policy_mapping_fn(1) + "_broke_rules"] = 0
        episode.custom_metrics[policy_mapping_fn(0) + "_broke_rules"] = 0
        episode.custom_metrics[policy_mapping_fn(1) + "_broke_rules"] = 0
        episode.custom_metrics[policy_mapping_fn(0) + "_finished_and_won"] = 0
        episode.custom_metrics[policy_mapping_fn(1) + "_finished_and_won"] = 0
        episode.custom_metrics[policy_mapping_fn(0) + "_total_wins"] = 0
        episode.custom_metrics[policy_mapping_fn(1) + "_total_wins"] = 0
        episode.custom_metrics[policy_mapping_fn(0) + "_total_losses"] = 0
        episode.custom_metrics[policy_mapping_fn(1) + "_total_losses"] = 0
        episode.custom_metrics["ties"] = 0
        episode.custom_metrics["broke_rules"] = 0
        episode.custom_metrics["finished"] = 0

    infos = episode.last_info_for(0)

    if "broke_rules" in infos:
        episode.custom_metrics[policy_mapping_fn(infos["broke_rules"]) + "_broke_rules"] += 1
        episode.custom_metrics[policy_mapping_fn(infos["broke_rules"]) + "_broke_rules"] += 1
        episode.custom_metrics[policy_mapping_fn(infos["broke_rules"]) + "_total_losses"] += 1
        episode.custom_metrics[policy_mapping_fn(get_other(infos["broke_rules"])) + "_total_wins"] += 1
        episode.custom_metrics["broke_rules"] += 1

    if "finished" in infos:
        episode.custom_metrics["finished"] += 1

        if infos["finished"] == "tie":
            episode.custom_metrics["ties"] += 1
        else:
            episode.custom_metrics[infos["finished"] + "_total_wins"] = 0
            episode.custom_metrics[infos["finished"] + "_finished_and_won"] = 0
            episode.custom_metrics[get_other(infos["finished"]) + "_total_losses"] = 0


def get_ppo_train(name, pols, env, logdir):
    config = {
        "multiagent": {
            "policies": pols,
            "policy_mapping_fn": policy_mapping_fn,
            "policies_to_train": [name],
        },
        # disable filters, otherwise we would need to synchronize those
        # as well to the DQN agent
        "observation_filter": "NoFilter",
        "callbacks": {
            "on_train_result": on_episode_end
        }}
    return PPOTrainer(env=env,
        config=config, logger_creator=lambda _ : UnifiedLogger(config, logdir))


def get_pg_train(name, pols, env, logdir):
    config = {
            "gamma": 0.95,
            "sample_batch_size": 2000,
            "multiagent": {
                "policies": pols,
                "policy_mapping_fn": policy_mapping_fn,
                "policies_to_train": [name],
            },
            # disable filters, otherwise we would need to synchronize those
            # as well to the DQN agent
            "observation_filter": "NoFilter",
            "callbacks": {
                "on_episode_end": on_episode_end
            }
        }
    return PGTrainer(
        env=env,
        config=config, logger_creator=lambda _ : UnifiedLogger(config, logdir))


def get_a3c_train(name, pols, env, logdir):
    config = {
        "multiagent": {
            "policies": pols,
            "policy_mapping_fn": policy_mapping_fn,
            "policies_to_train": [name],
        },
        # disable filters, otherwise we would need to synchronize those
        # as well to the DQN agent
        "observation_filter": "NoFilter",
        "callbacks": {
            "on_train_result": on_episode_end
        }}
    return A3CTrainer(
        env=env,
        config=config, logger_creator=lambda _: UnifiedLogger(config, logdir))


def get_ppo_pol(obs_space, act_space):
    return PPOTFPolicy, obs_space, act_space, {}


def get_pg_pol(obs_space, act_space):
    return PGTFPolicy, obs_space, act_space, {}


def get_hierarchical_pg_pol(obs_space, act_space, redundant_trainable=True, core_trainable=True, num_redundant_moves=2,
                            core_policy_hiddens=(64, 64,), redundant_policy_hiddens=(64, 64,)):

    return FreezablePGTFPolicy, obs_space, act_space, {"model":
                {
                    "custom_model": "freezable_modular_policy",
                    "custom_options": {
                        "redundant_trainable": redundant_trainable,
                        "core_trainable": core_trainable,
                        "num_redundant_moves": num_redundant_moves,
                        "core_policy_hiddens": core_policy_hiddens,
                        "redundant_policy_hiddens": redundant_policy_hiddens
                    }
                }}


def get_a3c_pol(obs_space, act_space):
    return A3CTFPolicy, obs_space, act_space, {}


ALG_POL_MAP = {"pg": get_pg_pol, "ppo": get_ppo_pol, "pg_hierarchical": get_hierarchical_pg_pol, "a3c": get_a3c_pol,
               "pg_hierarchical_core_frozen": partial(get_hierarchical_pg_pol, core_trainable=False),
               "pg_hierarchical_redundant_frozen": partial(get_hierarchical_pg_pol, redundant_trainable=False)}

ALG_TRAIN_MAP = {"pg": get_pg_train, "ppo": get_ppo_train, "pg_hierarchical": get_pg_train, "a3c": get_a3c_train,
                 "pg_hierarchical_core_frozen": get_pg_train, "pg_hierarchical_redundant_frozen": get_pg_train}


def get_policies(algs, obs_space, act_space):
    return {policy_mapping_fn(i): ALG_POL_MAP[alg](obs_space, act_space) for i, alg in enumerate(algs)}


def get_single_trainer(alg, pols, ind, env, logdir):
    return ALG_TRAIN_MAP[alg](policy_mapping_fn(ind), pols, env, logdir)


def get_algs(algs, obs_space, act_space, env, logdir):
    pols = get_policies(algs, obs_space, act_space)
    return [get_single_trainer(alg, pols, i, env, logdir) for i, alg in enumerate(algs)], pols
