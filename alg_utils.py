from ray.rllib.agents.pg.pg import PGTrainer
from ray.rllib.agents.pg.pg_policy import PGTFPolicy
from ray.rllib.agents.ppo.ppo import PPOTrainer
from ray.rllib.agents.ppo.ppo_policy import PPOTFPolicy
from ray.rllib.models import ModelCatalog
from freezable_policy import FreezableModularPolicy

def policy_mapping_fn(agent_id):
    return "agent"+str(agent_id % 2+1)+"_policy"


def get_ppo_train(name, pols, env):
    return PPOTrainer(
        env=env,
        config={
            "multiagent": {
                "policies": pols,
                "policy_mapping_fn": policy_mapping_fn,
                "policies_to_train": [name],
            },
            # disable filters, otherwise we would need to synchronize those
            # as well to the DQN agent
            "observation_filter": "NoFilter",
        })


def get_pg_train(name, pols, env):
    return PGTrainer(
        env=env,
        config={
            "multiagent": {
                "policies": pols,
                "policy_mapping_fn": policy_mapping_fn,
                "policies_to_train": [name],
            },
            # disable filters, otherwise we would need to synchronize those
            # as well to the DQN agent
            "observation_filter": "NoFilter",
        })

def get_pg_train_custom(name, pols, env, num_redundant_moves=2, train_core=True, train_redundant=True):
    ModelCatalog.register_custom_model("freezable_modular_policy", FreezableModularPolicy)

    return PGTrainer(
        env=env,
        config={
            "multiagent": {
                "policies": pols,
                "policy_mapping_fn": policy_mapping_fn,
                "policies_to_train": [name],
            },
            # disable filters, otherwise we would need to synchronize those
            # as well to the DQN agent
            "observation_filter": "NoFilter",
            "model":
                {
                    "custom_model": "freezable_modular_policy",
                    "custom_options": {
                        "redundant_trainable": train_redundant,
                        "core_trainable": train_core,
                        "num_redundant_moves": num_redundant_moves,
                        "core_policy_hiddens": [50, 25],
                        "redundant_policy_hiddens": [100, 25]
                    }
                }
        })



def get_ppo_pol(obs_space, act_space):
    return PPOTFPolicy, obs_space, act_space, {}


def get_pg_pol(obs_space, act_space):
    return PGTFPolicy, obs_space, act_space, {}



ALG_POL_MAP = {"pg": get_pg_pol, "ppo": get_ppo_pol, "pg_custom": get_pg_pol}
ALG_TRAIN_MAP = {"pg": get_pg_train, "ppo": get_ppo_train, "pg_custom": get_pg_train_custom}


def get_algs(algs, obs_space, act_space, env):
    pols = {policy_mapping_fn(i): ALG_POL_MAP[alg](obs_space, act_space) for i, alg in enumerate(algs)}

    return [ALG_TRAIN_MAP[alg](policy_mapping_fn(i), pols, env) for i, alg in enumerate(algs)], pols


def get_new_train(alg, env, pols, index):
    return ALG_TRAIN_MAP[alg](policy_mapping_fn(index), pols, env)
