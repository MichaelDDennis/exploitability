## Overview

At a high level, the pipeline of generating results for this project goes like: 

1. Train baseline agents using multi-agent, simultaneous-training RL
2. Train agents that are hardened using adversarial training (by performing k steps of training, then training an adversary against the current 
state of the agent, then training more, then training a new adversary, etc)
3. Exaluate the extent to which both baseline and hardened policies are vulnerability to a new adversary being optimized against them 


## Glossary
Throughout function names and documentation, this project uses terminology in a particular way. These are defined in various places in the paper and otherwise, but for the sake of 
having one centralized place where things can be found.   
**Opponent** - A general term for an agent playing against a player in a two-player game  
**Adversary** - An opponent that is optimized against against a specific player whose weights are kept fixed throughout adversary training. In this sense, a given 
adversary is particular to the state of the agent as it was when the adversary was training.  
**Core Action** - The action a player takes from the perspective of the strategic assessment of the game. In analogic terms, it doesn't 
matter how you write an X in tic-tac-toe, if it's still an X, it has the same effect strategically. Also sometimes referred to as "high-level action"  
**Redundant Action** - The degrees of freedom a player has in the execution of their core action. Continuing the analogy above, the redundant component 
of the action would be the differences in the ways you write an X or an O.  


