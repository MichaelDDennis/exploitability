## Overview

At a high level, the pipeline of generating results for this project goes like: 

1. Train baseline agents using multi-agent, simultaneous-training RL
2. Train agents that are hardened using adversarial training (by performing k steps of training, then training an adversary against the current 
state of the agent, then training more, then training a new adversary, etc)
3. Exaluate the extent to which both baseline and hardened policies are vulnerability to a new adversary being optimized against them 


## Glossary
Throughout function names and documentation, this project uses terminology in a particular way. These are defined in various places in the paper and otherwise, but for the sake of 
having one centralized place where things can be found.   
  
**Opponent** - A general term for an agent playing against a player in a two-player game  
**Adversary** - An opponent that is optimized against against a specific player whose weights are kept fixed throughout adversary training. In this sense, a given 
adversary is particular to the state of the agent as it was when the adversary was training.  
**Core Action** - The action a player takes from the perspective of the strategic assessment of the game. In analogic terms, it doesn't 
matter how you write an X in tic-tac-toe, if it's still an X, it has the same effect strategically. Also sometimes referred to as "high-level action"  
**Redundant Action** - The degrees of freedom a player has in the execution of their core action. Continuing the analogy above, the redundant component 
of the action would be the differences in the ways you write an X or an O.  


## Technical Background
This project uses [Sacred](https://github.com/IDSIA/sacred) to configure its experiments, so if you see the `with` syntax in a command line command, that means that either 
a Sacred named config or direct configuration parameter is being passed in. 

All training is done using [RLLib](https://ray.readthedocs.io/en/latest/rllib.html), a reinforcement learning project within the broader distributed computing framework of Ray. We chose to use this 
framework because of its strong existing support for multi-agent RL training. 

## Pretrained Models 
All of our training checkpoints can be found at `s3://systemic-strategic-vulnerability-checkpoints`, which should be 
publicly accessibly via the AWS command line utilities.  

## Replication 
First, install the source package locally (we recommend in a virtualenv) by calling `pip install -e .`  

### Training baseline agents 
```python -m exploitability.training.marl_training with paper_config_baseline_train```

This will by default save trained agents into a directory `baseline_train`

### Adversarial hardening 
Here, `directory_of_trained_agents` should be set to `baseline_train`. The `prev_run` parameter should be to set to whichever 
numbered directory within `baseline_train` corresponds to the agent you'd like to harden. If prev_run is not passed in, 
measure_exploitability will automatically use the latest training run within `baseline_train`.  The sacred configurations
``paper_config_itr_adv_train_tot``, ``paper_config_itr_adv_train_sys``, ``paper_config_itr_adv_train_str`` set the default
configurations for total, systemic, or strategic adversarial hardening respectively.

```
python -m exploitability.training.measure_exploitability with paper_config_itr_adv_train_tot dir_loc=directory_of_trained_agents prev_run=agent_num
python -m exploitability.training.measure_exploitability with paper_config_itr_adv_train_sys dir_loc=directory_of_trained_agents prev_run=agent_num
python -m exploitability.training.measure_exploitability with paper_config_itr_adv_train_str dir_loc=directory_of_trained_agents prev_run=agent_num
```


### Measuring vulnerability of policies
To measure vulnerability of all three kinds addressed by this paper - systemic, strategic, and total - you can use one single 
script, `test_all_vul.sh`, and pass in parameters specifying what policy you want to test the vulnerability of.

`bash scripts/test_all_vul.sh "directory_of_trained_agents" agent_num victim_policy_key`

In the invocation above, victim_policy_key is a string tag used to identify the victim policy that you're training an adversary 
against, for the sake of the ability of later steps in the pipeline to categorize different runs. For example, to run 
vulnerability testing against baseline policies as well as ones against each type of adversary (as we did in the paper), you could call: 
```
bash scripts/test_all_vul.sh baseline_training <baseline_index> baseline
bash scripts/test_all_vul.sh exploitability_testing <iat_sys_index> iat_sys
bash scripts/test_all_vul.sh exploitability_testing <iat_strat_index> iat_strat
bash scripts/test_all_vul.sh exploitability_testing <iat_tot_index> iat_tot
```
Where all of the angle-bracketed terms refer to the numeric sacred run ID that corresponds to the run you want to test 
within the broader directory. Here, iat_sys, iat_strat, and iat_tot refer to policy that were hardened (via Iterative Adversarial Training)
to systemic, strategic, or total attackers respectively. Baseline is, as you would expect, the baseline unhardened policy. 

These experiements will run in the background with no visible output.  You can monitor progress by looking at the sacred
directory at ``exploitability_testing/<most_recent_sacred_run>/run.json``

### Graphing Results
To run the full suite of graphing code, you can use the invocation: 
```
python -m exploitability.analysis.graph_ray_results episode_length_training_curve with policy_dir=baseline_training
python -m exploitability.analysis.graph_ray_results exploitability_bar_chart with policy_dir=exploitability_testing
python -m exploitability.analysis.graph_ray_results exploitability_curves with policy_dir=exploitability_testing
```

This will produce:  
1. A plot of episode length as a function of training during original multi-agent training, used as a measure of how much both policies together are improving.  
2. A bar chart showing the vulnerability of various policies after an adversary has been fully trained against them.  
3. A plot showing the reward of the adversary over the course of training against a fixed victim 
